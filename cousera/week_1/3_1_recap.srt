1
00:00:03.150 --> 00:00:06.265
ねえ皆さん。このビデオでは、

2
00:00:06.265 --> 00:00:11.460
私は基本的な機械学習のアプローチとその背
後にあるアイデアの簡単な概要を実行したい
。

3
00:00:11.460 --> 00:00:15.480
私が復習したい機械学習アルゴリズムのいく
つかの有名なものがあります。

4
00:00:15.480 --> 00:00:16.765
線形モデルだ

5
00:00:16.765 --> 00:00:20.220
木ベースのメソッド、k-最も近い隣人、お
よびニューラルネット。

6
00:00:20.220 --> 00:00:21.675
この家族のそれぞれにとって、

7
00:00:21.675 --> 00:00:25.295
私は例との短い直感的な説明を与える。

8
00:00:25.295 --> 00:00:27.660
これらのトピックのいずれかを覚えていない
場合は、

9
00:00:27.660 --> 00:00:32.630
私は強く、追加の資料からのリンクを使用し
て学ぶことをお勧めします。

10
00:00:32.630 --> 00:00:34.910
線形モデルから始めましょう。

11
00:00:34.910 --> 00:00:37.648
我々はポイントの2つのセットを持っている
ことを想像し、

12
00:00:37.648 --> 00:00:41.640
グレーポイントは、別のクラスと緑のものに
属しています。

13
00:00:41.640 --> 00:00:45.405
それはラインとそれらを分けることは非常に
直観的である。

14
00:00:45.405 --> 00:00:49.810
この場合、2次元の点しかないので、非常に
簡単です。

15
00:00:49.810 --> 00:00:53.620
しかし、このアプローチは、高次元空間の一
般化することができます。

16
00:00:53.620 --> 00:00:56.410
これは線形モデルの背後にある主な考え方で
す。

17
00:00:56.410 --> 00:01:01.340
彼らは2つの部分にスペースを分割する平面
とオブジェクトを分離しようとします。

18
00:01:01.340 --> 00:01:07.700
ロジスティック回帰や SVM など、この
モデルクラスのいくつかの例を覚えておくこ
とができます。

19
00:01:07.700 --> 00:01:11.535
それらはすべて異なった損失機能の線形モデ
ルである。

20
00:01:11.535 --> 00:01:13.700
私は線形モデルがあることを強調したいと思
う

21
00:01:13.700 --> 00:01:17.005
特に疎な高次元データに適しています。

22
00:01:17.005 --> 00:01:20.295
しかし、線形モデルの限界を念頭におく必要
があります。

23
00:01:20.295 --> 00:01:24.175
多くの場合、ポイントはこのような単純なア
プローチでは分離できません。

24
00:01:24.175 --> 00:01:28.315
例として、リングを形成するポイントの2つ
のセットを想像することができます

25
00:01:28.315 --> 00:01:30.085
1つは、他の内側。

26
00:01:30.085 --> 00:01:33.005
どのようにそれらを分離することはかなり明
白だが、

27
00:01:33.005 --> 00:01:37.490
線形モデルは適切な選択ではなく、この場合
は失敗します。

28
00:01:37.490 --> 00:01:42.545
線形モデルの実装は、ほぼすべての機械学習
ライブラリで見つけることができます。

29
00:01:42.545 --> 00:01:45.570
Scikit で最も知られている実装-
ライブラリを学ぶ。

30
00:01:45.570 --> 00:01:49.120
我々の注目に値するもう一つの実装は、Vo
wpal Wabbit です。

31
00:01:49.120 --> 00:01:52.585
それは本当に大きなデータセットを処理する
ように設計されているため。

32
00:01:52.585 --> 00:01:57.485
ここでは線形モデルを使い、次のファミリ、
ツリーベースのメソッドに移ります。

33
00:01:57.485 --> 00:02:00.210
ツリーベースのメソッドは、デシジョンツリ
ーを

34
00:02:00.210 --> 00:02:03.175
より複雑なモデルを構築するための基本ブロ
ック。

35
00:02:03.175 --> 00:02:06.185
デシジョンツリーのしくみの例を考えてみま
しょう。

36
00:02:06.185 --> 00:02:10.305
我々は、線形のケースに似てポイントの2つ
のセットを持っていることを想像してくださ
い。

37
00:02:10.305 --> 00:02:15.995
もう1つのクラスを、軸の1つに平行な線で
区切ってみましょう。

38
00:02:15.995 --> 00:02:19.290
このような制限を使用して、その数を大幅に
削減します。

39
00:02:19.290 --> 00:02:23.630
可能な行と私たちは簡単な方法で行を記述す
ることができます。

40
00:02:23.630 --> 00:02:26.253
その図に示すように分割を設定した後、

41
00:02:26.253 --> 00:02:27.985
我々は、2つのサブスペースを取得します

42
00:02:27.985 --> 00:02:30.665
上にはグレー = 1
の確率がありますが、

43
00:02:30.665 --> 00:02:34.005
と下限は、灰色 = 0.2
の確率を持つことになります。

44
00:02:34.005 --> 00:02:37.330
上側のサブスペースはそれ以上の分割を必要
としません。

45
00:02:37.330 --> 00:02:40.300
下位サブスペースの分割を続けてみましょう
。

46
00:02:40.300 --> 00:02:45.750
今、我々は、左側のサブスペースと右側の1
つのグレーの確率をゼロにしています。

47
00:02:45.750 --> 00:02:49.220
これは、デシジョンツリーのしくみについて
簡単に説明したものです。

48
00:02:49.220 --> 00:02:53.950
これは、サブスペースにサブ分割スペースを
繰り返すに分割し、征服のアプローチを使用
しています。

49
00:02:53.950 --> 00:02:57.770
直感的に、単一のデシジョンツリーは、スペ
ースを分割するように想像することができま
す

50
00:02:57.770 --> 00:03:02.500
ボックス内の定数を使用してデータを近似し
ます。

51
00:03:02.500 --> 00:03:05.410
真の軸分割と対応する定数の方法

52
00:03:05.410 --> 00:03:08.525
デシジョンツリーを構築するためのいくつか
のアプローチを生成します。

53
00:03:08.525 --> 00:03:12.695
また、そのような木は多くの方法で一緒に組
み合わせることができます。

54
00:03:12.695 --> 00:03:16.338
これにより、さまざまなツリーベースのアル
ゴリズムにつながり、

55
00:03:16.338 --> 00:03:21.005
ほとんどのそれらのランダムな森林と勾配の
決定木を後押しされて有名。

56
00:03:21.005 --> 00:03:23.075
それが何なのかわからない場合は

57
00:03:23.075 --> 00:03:27.675
私は強く、これらのトピックを追加資料から
のリンクを使用して覚えてお勧めします。

58
00:03:27.675 --> 00:03:30.515
一般的に、ツリーベースのモデルは非常に強
力です。

59
00:03:30.515 --> 00:03:33.320
、表形式データの既定の方法として適してい
ます。

60
00:03:33.320 --> 00:03:35.055
ほぼすべての競技では、

61
00:03:35.055 --> 00:03:37.315
勝者はこのアプローチを使用します。

62
00:03:37.315 --> 00:03:39.290
ただし、ツリーベースのメソッドの場合は、

63
00:03:39.290 --> 00:03:44.870
多くの分割が必要なため、線形依存関係をキ
ャプチャするのは困難です。

64
00:03:44.870 --> 00:03:49.535
我々は、ラインで区切ることができるポイン
トの2つのセットを想像することができます
。

65
00:03:49.535 --> 00:03:54.765
この場合、ポイントを分けるために、分割の
多いツリーを成長させる必要があります。

66
00:03:54.765 --> 00:03:58.965
このような場合でも、私たちの木は、決定の
境界線の近くに不正確かもしれない

67
00:03:58.965 --> 00:04:00.640
図に示すように。

68
00:04:00.640 --> 00:04:02.731
線形モデルと同様に、

69
00:04:02.731 --> 00:04:04.030
の実装を見つけることができます

70
00:04:04.030 --> 00:04:07.225
ほとんどすべての機械学習ライブラリのツリ
ーベースのモデル。

71
00:04:07.225 --> 00:04:10.010
Scikit-
学ぶ非常に良い実装が含まれています

72
00:04:10.010 --> 00:04:12.940
私は個人的に好むランダムフォレスト。

73
00:04:12.940 --> 00:04:17.385
すべての Scikit-学習勾配ブースト
デシジョンツリーの実装が含まれています。

74
00:04:17.385 --> 00:04:23.260
私は彼らのより高い速度と精度のために
XGBoost と LightGBM
のようなライブラリを使用することを好む。

75
00:04:23.260 --> 00:04:28.775
そこで、ここでは、ツリーベースのメソッド
の概要を終了し、k-NN に移動します。

76
00:04:28.775 --> 00:04:30.940
説明を始める前に

77
00:04:30.940 --> 00:04:35.040
私は k-NN は
k-最も近い隣人の略であると言いたい。

78
00:04:35.040 --> 00:04:37.815
1つは、ニューラルネットワークでそれをミ
ックスしないでください。

79
00:04:37.815 --> 00:04:41.790
そこで、おなじみのバイナリー分類問題につ
いて見てみましょう。

80
00:04:41.790 --> 00:04:44.250
我々はラベルを予測する必要があることを想
像してください

81
00:04:44.250 --> 00:04:47.005
このスライドの疑問符で表示されるポイント
。

82
00:04:47.005 --> 00:04:52.765
我々は、互いに近い点が類似したラベルを持
っている可能性があると仮定します。

83
00:04:52.765 --> 00:04:55.558
だから、我々は最も近いポイントを見つける
必要があります

84
00:04:55.558 --> 00:04:59.740
を矢印で表示し、そのラベルを回答として選
択します。

85
00:04:59.740 --> 00:05:03.195
これは、最も近い隣人の方法が一般的に動作
する方法です。

86
00:05:03.195 --> 00:05:05.350
それは簡単に一般化することができます
k-NN は、

87
00:05:05.350 --> 00:05:10.685
我々は k-最も近いオブジェクトを検索し
、プラス多数決でラベルを選択した場合。

88
00:05:10.685 --> 00:05:14.270
k-NN
の背後にある直感は非常に簡単です。

89
00:05:14.270 --> 00:05:17.250
近いオブジェクトは、同じラベルを持つ可能
性があります。

90
00:05:17.250 --> 00:05:19.165
この特定の例では、

91
00:05:19.165 --> 00:05:22.350
我々は、最も近いオブジェクトを見つけるた
めに正方形の距離を使用します。

92
00:05:22.350 --> 00:05:26.175
一般的には、このような距離関数を使用して
も無意味な場合があります。

93
00:05:26.175 --> 00:05:31.630
たとえば、画像の上の正方形の距離は意味を
キャプチャすることはできません。

94
00:05:31.630 --> 00:05:34.070
アプローチのシンプルさにもかかわらず、

95
00:05:34.070 --> 00:05:37.520
最も近い隣人に基づく特徴は頻繁に非常に報
知的である。

96
00:05:37.520 --> 00:05:41.065
詳細については、後のコースで説明します。

97
00:05:41.065 --> 00:05:45.830
k-NN の実装は、多くの機械学習ライブ
ラリで見つけることができます。

98
00:05:45.830 --> 00:05:49.607
私は Scikit
から実装を使用することをお勧め-
それを使用してから学ぶ

99
00:05:49.607 --> 00:05:52.240
高速化のためのアルゴリズム行列の記憶と

100
00:05:52.240 --> 00:05:55.740
複数の定義済み距離関数を使用できます。

101
00:05:55.740 --> 00:05:59.195
また、それはあなた自身の距離関数を実装す
ることができます。

102
00:05:59.195 --> 00:06:03.520
私は概要にしたいモデルの次の大きなクラス
は、ニューラルネットワークです。

103
00:06:03.520 --> 00:06:07.115
ニューラルネットは、機械学習モデルの特別
なクラスです,

104
00:06:07.115 --> 00:06:09.310
これは別のトピックに値する。

105
00:06:09.310 --> 00:06:12.425
一般的に、このような方法は、このブラック
ボックスで見ることができますを生成する

106
00:06:12.425 --> 00:06:16.570
デシジョンツリーとは対照的に滑らかな分離
曲線。

107
00:06:16.570 --> 00:06:20.390
私はあなたがスライドに示されている
TensorFlow
の遊び場を訪問することをお勧め、

108
00:06:20.390 --> 00:06:23.810
と簡単なフィードフォワードネットワークの
さまざまなパラメータを使用して再生

109
00:06:23.810 --> 00:06:28.085
どのようにフィードフォワードニューラルネ
ットの動作についていくつかの直感を得るた
めに。

110
00:06:28.085 --> 00:06:32.242
ニューラルネットのいくつかのタイプは、特
に画像に適しています,

111
00:06:32.242 --> 00:06:35.085
サウンド、テキスト、およびシーケンス。

112
00:06:35.085 --> 00:06:38.385
このコースではニューラルネットの詳細につ
いては説明しません。

113
00:06:38.385 --> 00:06:42.400
ニューラルネットはここ数年、多くの注目を
集めて以来、

114
00:06:42.400 --> 00:06:44.935
それらを使用するためのフレームワークがた
くさんあります。

115
00:06:44.935 --> 00:06:47.360
TensorFlow
のようなパッケージ、ハード、

116
00:06:47.360 --> 00:06:52.070
MXNet、PyTorch、およびラザニ
アは、ニューラルネットを供給するために使
用することができます。

117
00:06:52.070 --> 00:06:54.817
以来、それを提供する私は個人的に
PyTorch を好む

118
00:06:54.817 --> 00:06:58.190
複雑なネットワークを定義する柔軟で使いや
すい方法。

119
00:06:58.190 --> 00:06:59.865
この簡単な要約の後、

120
00:06:59.865 --> 00:07:03.520
私は自由な昼食の定理についての少数の単語
を言いたいと思う。

121
00:07:03.520 --> 00:07:07.290
基本的に、自由な昼食の定理はないと述べて
いる

122
00:07:07.290 --> 00:07:11.235
すべてのタスクの他のすべてのアウトパフォ
ームメソッドは、

123
00:07:11.235 --> 00:07:14.040
つまり、あらゆる方法について、

124
00:07:14.040 --> 00:07:19.395
我々は、この特定のメソッドが最適ではない
タスクを構築することができます。

125
00:07:19.395 --> 00:07:24.505
その理由は、すべてのメソッドがデータまた
はタスクに関するいくつかの仮定に依存して
いるからです。

126
00:07:24.505 --> 00:07:26.640
これらの仮定が失敗した場合、

127
00:07:26.640 --> 00:07:28.470
制限が不十分に実行されます。

128
00:07:28.470 --> 00:07:33.570
私たちにとって、これは1つのアルゴリズム
だけではすべての競争ができないことを意味
します。

129
00:07:33.570 --> 00:07:37.705
だから我々はさまざまな仮定をオフに基づい
てツールの様々を持っている必要があります
。

130
00:07:37.705 --> 00:07:39.510
このビデオの終了前に、

131
00:07:39.510 --> 00:07:42.477
私はあなたの Scikit
からの例を紹介したい-ライブラリを学ぶ

132
00:07:42.477 --> 00:07:46.115
異なる分類子の決定サーフェスをプロットし
ます。

133
00:07:46.115 --> 00:07:48.706
我々は、アルゴリズムの種類を見ることがで
きます

134
00:07:48.706 --> 00:07:52.940
決定の境界の重要な影響とその結果
[聞こえない]。

135
00:07:52.940 --> 00:07:57.170
私は強く、この例に深く飛び込むことをお勧
めしていることを確認

136
00:07:57.170 --> 00:08:01.635
これらの分類器がそのような表面をなぜ作り
出すか直観を持ちなさい。

137
00:08:01.635 --> 00:08:05.700
最後に、私はあなたにこのビデオの主なポイ
ントを思い出させたい。

138
00:08:05.700 --> 00:08:08.590
まず第一に、銀の弾丸アルゴリズムはありま
せん

139
00:08:08.590 --> 00:08:12.840
すべてとすべてのタスクの他のすべてを上回
る。

140
00:08:12.840 --> 00:08:15.714
次に、線形モデルが分裂として想像すること
ができることである

141
00:08:15.714 --> 00:08:19.485
ハイパープレーンで区切られた2つのサブス
ペースにスペース。

142
00:08:19.485 --> 00:08:26.145
ツリーベースのメソッドは、スペースをボッ
クスに分割し、各ボックスの予測を定数で使
用します。

143
00:08:26.145 --> 00:08:29.250
k-NN
のメソッドは、仮定に基づいています

144
00:08:29.250 --> 00:08:32.220
オブジェクトを閉じると、同じラベルを持つ
可能性があります。

145
00:08:32.220 --> 00:08:35.685
そこで、最も近いオブジェクトを見つけ、ラ
ベルを選択する必要があります。

146
00:08:35.685 --> 00:08:40.075
また、k-NN のアプローチは大きくポイ
ントの近さを測定する方法に依存しています
。

147
00:08:40.075 --> 00:08:43.135
フィードフォワード型ニューラルネットの解
釈が困難

148
00:08:43.135 --> 00:08:46.580
しかし、彼らは滑らかな非線形意思決定の境
界を生成します。

149
00:08:46.580 --> 00:08:51.630
最も強力な方法は、グラデーションブースト
デシジョンツリーとニューラルネットワーク
です。

150
00:08:51.630 --> 00:08:53.595
しかし、我々は線形モデルを過小評価しない
でください

151
00:08:53.595 --> 00:08:56.795
と k-NN
は時々、彼らは良いかもしれないので。

152
00:08:56.795 --> 00:09:02.350
このコースでは、後で関連する例を紹介しま
す。ご注意をありがとうございました。

