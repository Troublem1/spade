1
00:00:00.000 --> 00:00:03.844
我々は StackNet
との議論を継続することができます。

2
00:00:03.844 --> 00:00:09.106
StackNet は、スケーラブルなメタ
モデリングの方法論を利用しています

3
00:00:09.106 --> 00:00:15.760
複数のレベルのニューラルネットワークアー
キテクチャで複数のモデルを結合するスタッ
キング。

4
00:00:15.760 --> 00:00:19.503
これは、同じレベル内のため、スケーラブル
です

5
00:00:19.503 --> 00:00:22.986
すべてのモデルを並行して実行することがで
きます。

6
00:00:22.986 --> 00:00:26.540
それはの使用を作るので積み重ねを利用する

7
00:00:26.540 --> 00:00:30.775
我々はデータを分割する前に述べたこの手法
は、

8
00:00:30.775 --> 00:00:33.995
我々は、いくつかのデータを保持する予測を
行う

9
00:00:33.995 --> 00:00:39.430
そして、我々はそれらの予測を訓練するため
に別のモデルを使用します。

10
00:00:39.430 --> 00:00:41.525
そして、我々は後に表示されますように、

11
00:00:41.525 --> 00:00:46.620
これは、ニューラルネットワークの多くに似
ています。

12
00:00:46.620 --> 00:00:54.000
今、私たちは、学生や教師との前に与えたそ
の素朴な例を続けてみましょう

13
00:00:54.000 --> 00:00:57.634
概念的には、現実の世界では、理解するため
に、

14
00:00:57.634 --> 00:01:02.270
別のレイヤーを追加する必要があります。

15
00:01:02.270 --> 00:01:03.630
だからその例では、

16
00:01:03.630 --> 00:01:06.870
私たちは、彼女が答えを結合しようとしてい
た教師を持っている

17
00:01:06.870 --> 00:01:13.560
別の学生と彼女は、特定の仮定の下で17の
推定値を出力していた。

18
00:01:13.560 --> 00:01:18.885
私たちはより多くのメタ学習者を導入するこ
とで、この例をより面白くすることができま
す。

19
00:01:18.885 --> 00:01:21.345
彼を RF さんと呼びましょう

20
00:01:21.345 --> 00:01:23.395
物理学の先生でもあります。

21
00:01:23.395 --> 00:01:29.305
氏 RF は、LR がアンサンブルに大き
な貢献を持っている必要がありますと考えて
いる

22
00:01:29.305 --> 00:01:31.810
彼はプライベートレッスンをしているので

23
00:01:31.810 --> 00:01:35.920
彼と彼はそれほど離れていないことを知って
いる。

24
00:01:35.920 --> 00:01:40.360
だから彼はわずかに異なる方法からデータを
見ることができます

25
00:01:40.360 --> 00:01:46.330
これらの予測のさまざまな部分を活用し、別
の見積もりを作成します。

26
00:01:46.330 --> 00:01:51.905
一方、教師はそれを回避し、平均を取ること
ができる、

27
00:01:51.905 --> 00:02:00.970
我々は、作成することができるか、我々はこ
こにモデルのより高い権威や別の層を導入す
ることができます。

28
00:02:00.970 --> 00:02:03.375
校長と呼びましょう

29
00:02:03.375 --> 00:02:07.540
GBM、買物をするために、よりよい予言を
しなさい。

30
00:02:07.540 --> 00:02:12.935
と GBM は、学生が与えている答えを知
る必要はありません。

31
00:02:12.935 --> 00:02:16.413
彼が知る必要がある唯一のことは、教師から
の入力です。

32
00:02:16.413 --> 00:02:17.500
そしてこの場合、

33
00:02:17.500 --> 00:02:26.994
彼は16.2 予測を出力することによって
彼の物理学の教師を信頼するために熱心です
。

34
00:02:26.994 --> 00:02:29.100
なぜこれは人々に使用されるのだろうか?

35
00:02:29.100 --> 00:02:32.235
つまり、それはすでに複雑ではありませんか
?

36
00:02:32.235 --> 00:02:36.810
何でそんなに複雑なことをしようとするんだ
?

37
00:02:36.810 --> 00:02:43.155
私のチームが使ったコンクールの例をあげて
いるのですが、

38
00:02:43.155 --> 00:02:45.851
積み重ねの4層、

39
00:02:45.851 --> 00:02:47.690
勝つために。

40
00:02:47.690 --> 00:02:52.950
そして、我々は入力データの2つの異なるソ
ースを使用します。

41
00:02:52.950 --> 00:02:55.920
複数のモデルを生成しました。

42
00:02:55.920 --> 00:02:59.880
通常、出口ブーストとロジスティック回帰、

43
00:02:59.880 --> 00:03:07.235
そして、我々は、トップスコアを取得するた
めに、4層のアーキテクチャにそれらを与え
た。

44
00:03:07.235 --> 00:03:13.385
4層目を使わずに脱出できたとはいえ

45
00:03:13.385 --> 00:03:17.970
我々はまだ勝つためにレベル3までそれを必
要とする。

46
00:03:17.970 --> 00:03:24.340
したがって、深い積み重ねの展開の有用性を
理解することができます。

47
00:03:24.340 --> 00:03:31.365
もう1つの例では、Homesite
の競争 Homesite
保険によって組織され、再び

48
00:03:31.365 --> 00:03:35.580
我々は、データの多くの異なるビューを作成
しました。

49
00:03:35.580 --> 00:03:38.010
従って私達は異なった変形を有した。

50
00:03:38.010 --> 00:03:40.465
多くのモデルを生成しました。

51
00:03:40.465 --> 00:03:46.600
私たちは3つのレベルのアーキテクチャにこ
れらのモデルを与えた。

52
00:03:46.600 --> 00:03:48.770
私たちは再び3番目の層を必要としなかった
と思う。

53
00:03:48.770 --> 00:03:54.770
おそらく2つのレベルだけで脱出できた

54
00:03:54.770 --> 00:03:58.395
勝つためには深い積み重ねが必要だった。

55
00:03:58.395 --> 00:04:00.270
そこであなたの答えですが、

56
00:04:00.270 --> 00:04:07.005
複数のレベルでの深い積み重ねは本当にあな
たが大会に勝つのに役立ちます。

57
00:04:07.005 --> 00:04:10.975
公平性と開放性の精神で、

58
00:04:10.975 --> 00:04:13.525
についていくつかの批判がされている

59
00:04:13.525 --> 00:04:18.060
多分彼らは商業的価値を持っていない大規模
なアンサンブル、

60
00:04:18.060 --> 00:04:21.055
彼らは機密性の高いです。

61
00:04:21.055 --> 00:04:24.180
私はその上に3つのことを追加する必要があ
ります。

62
00:04:24.180 --> 00:04:28.020
最初は、何が高価な今日の可能性が考えられ
ている

63
00:04:28.020 --> 00:04:32.280
高価な明日ではないと我々は、例えば、見て
きました

64
00:04:32.280 --> 00:04:33.840
深い学びとともに、

65
00:04:33.840 --> 00:04:35.670
ここで、gpu の出現で、

66
00:04:35.670 --> 00:04:42.295
彼らは100倍高速になっている今、彼らは
再び非常に、非常に人気となっている。

67
00:04:42.295 --> 00:04:43.845
もう一つは、

68
00:04:43.845 --> 00:04:48.420
あなたは、常に非常に構築する必要はありま
せん

69
00:04:48.420 --> 00:04:50.530
非常に深いアンサンブルが、まだ、

70
00:04:50.530 --> 00:04:53.180
小さなアンサンブルは、まだ本当に役立ちま
す。

71
00:04:53.180 --> 00:04:57.285
従ってそれらをする方法を知ることは企業に
価値を加えることができる

72
00:04:57.285 --> 00:05:02.245
もう一度、彼らが決定を望む速さについての
異なる仮定に基づいて、

73
00:05:02.245 --> 00:05:04.825
積み重ねてから見ることができる隆起はどの
くらいなのか、

74
00:05:04.825 --> 00:05:09.195
これは、時にはそれ以上の、いつか少ないで
すが異なる場合があります。

75
00:05:09.195 --> 00:05:13.070
そして、一般的に、どのくらいのコンピュー
ティングパワーを持っている。

76
00:05:13.070 --> 00:05:20.065
私達は複数の層の積み重ねが非常に有用であ
る場合もある場合を作ることができる。

77
00:05:20.065 --> 00:05:22.410
そして、最後のポイントは、これらは

78
00:05:22.410 --> 00:05:27.040
予測モデリングコンペティションだから、オ
リンピックのようなビットです。

79
00:05:27.040 --> 00:05:31.410
理論的に最高のあなたを見ることができるの
はうれしいです

80
00:05:31.410 --> 00:05:36.000
これは技術革新が引き継ぐ方法であるため、
得ることができます。

81
00:05:36.000 --> 00:05:37.740
これが我々の前進だ

82
00:05:37.740 --> 00:05:43.265
StackNet をニューラルネットワー
クとして表現することができます。

83
00:05:43.265 --> 00:05:46.955
普通はニューラルネットワークで

84
00:05:46.955 --> 00:05:51.110
我々は、彼らがいる隠しユニットのこれらの
アーキテクチャを持っている

85
00:05:51.110 --> 00:05:57.500
線形回帰の形式で入力と接続されます。

86
00:05:57.500 --> 00:06:01.830
だから実際には、それはかなり線形回帰のよ
うに見えます。

87
00:06:01.830 --> 00:06:04.700
あなたが係数のセットを持っているかどうか
だから、あなたが持っている

88
00:06:04.700 --> 00:06:08.275
母音ネットワークでバイアスと呼ぶ定数値

89
00:06:08.275 --> 00:06:11.420
そして、これはどのように出力予測のいずれ
かです

90
00:06:11.420 --> 00:06:14.652
それから取られる隠された単位、

91
00:06:14.652 --> 00:06:18.560
出力を作成するために収集されます。

92
00:06:18.560 --> 00:06:23.350
StackNet の概念は、実際にはそれ
ほど異なっていません。

93
00:06:23.350 --> 00:06:25.495
私たちがしたい唯一のことは、

94
00:06:25.495 --> 00:06:30.172
我々は、その線形回帰またはその知覚に限定
される必要はありません。

95
00:06:30.172 --> 00:06:34.230
我々は、任意の機械学習アルゴリズムを使用
できるようにしたい。

96
00:06:34.230 --> 00:06:38.520
それはさておき、アーキテクチャはまったく
同じでなければなりません。

97
00:06:38.520 --> 00:06:41.750
かなり似ているかもしれません。

98
00:06:41.750 --> 00:06:44.375
では、これを鍛えるには?

99
00:06:44.375 --> 00:06:48.254
典型的なニューラルネットワークでは、bi
propagation を使用します。

100
00:06:48.254 --> 00:06:50.135
ここでは、このコンテキストでは、

101
00:06:50.135 --> 00:06:51.980
これは実現不可能です。

102
00:06:51.980 --> 00:06:56.200
私はこのネットワークの仕事をしようとする
という文脈で意味する

103
00:06:56.200 --> 00:07:01.170
すべてが微分ではないので、任意の入力モデ
ルで。

104
00:07:01.170 --> 00:07:04.420
だから、私たちはスタッキングを使用するこ
とができます。

105
00:07:04.420 --> 00:07:09.845
ここでスタッキングは、出力をリンクする方
法です。

106
00:07:09.845 --> 00:07:13.985
予測、ノードの出力、ターゲット変数。

107
00:07:13.985 --> 00:07:21.605
これは、ノードを使用して入力フィーチャか
らリンクを作成する方法です。

108
00:07:21.605 --> 00:07:29.850
しかし、もしあなたが積み重ねが働く方法を
覚えれば、あなたは、いくつかの列車データ
を持っています。

109
00:07:29.850 --> 00:07:32.970
そして、あなたは2つの半分に分割する必要
があります。

110
00:07:32.970 --> 00:07:35.790
だから、最初の部分を使用すると呼ばれる、
トレーニング、

111
00:07:35.790 --> 00:07:39.800
と呼ばれる他の部分に予測を行うためには、
有効な。

112
00:07:39.800 --> 00:07:47.385
我々は、より多くの層を追加する私たちにい
くつかの隆起を与えると仮定する場合、

113
00:07:47.385 --> 00:07:49.938
またやりたかったら

114
00:07:49.938 --> 00:07:54.080
我々は、2つの部分に有効なデータを再分割
する必要があります。

115
00:07:54.080 --> 00:07:56.056
それを呼びましょう, ミニトレイン,
とミニ有効.

116
00:07:56.056 --> 00:07:58.000
そして、あなたはここで問題を見ることがで
きます。

117
00:07:58.000 --> 00:08:00.672
大きなデータがあると仮定して

118
00:08:00.672 --> 00:08:03.710
それから、これは本当に問題ではないかもし
れません。

119
00:08:03.710 --> 00:08:07.670
しかし、特定の状況では、我々は多くのデー
タを持っていない。

120
00:08:07.670 --> 00:08:15.375
理想的には、我々は常に再我々のデータを分
割することなく、これを行うにしたいと思い
ます。

121
00:08:15.375 --> 00:08:19.470
したがって、トレーニングデータセットを最
小化します。

122
00:08:19.470 --> 00:08:24.395
だから、これは我々が K
倍のパラダイムを使用する理由です。

123
00:08:24.395 --> 00:08:30.135
我々は、4つの機能を持つトレーニングデー
タセットを持っていると仮定してみましょう
x0, x1,

124
00:08:30.135 --> 00:08:36.520
x2、x3、y 変数、またはターゲット。

125
00:08:36.520 --> 00:08:39.827
k = 4 を使用している場合は、k 倍

126
00:08:39.827 --> 00:08:43.000
これは、ハイパーパラメータは、ここで何を
置くことです。

127
00:08:43.000 --> 00:08:49.250
これらのデータセットから4つの異なる部分
を作成します。

128
00:08:49.250 --> 00:08:52.285
ここで私は別の色を入れている、

129
00:08:52.285 --> 00:08:54.300
これらの部品のそれぞれに色。

130
00:08:54.300 --> 00:08:58.780
訓練を始めるためには

131
00:08:58.780 --> 00:09:05.190
我々は、行と同じサイズを持つ空のベクトル
を作成しますか、

132
00:09:05.190 --> 00:09:07.065
トレーニングデータのように、

133
00:09:07.065 --> 00:09:08.845
しかし、今のところは空です。

134
00:09:08.845 --> 00:09:13.115
それから、襞の一つ一つに対して、

135
00:09:13.115 --> 00:09:19.435
我々は、我々は、トレーニングデータのサブ
セットを取る開始します。

136
00:09:19.435 --> 00:09:25.555
この場合は、赤、黄、緑から始めます。

137
00:09:25.555 --> 00:09:27.230
我々は、モデルを訓練する

138
00:09:27.230 --> 00:09:30.115
そして、我々は、青色の部分を取る

139
00:09:30.115 --> 00:09:32.300
と予測を行います。

140
00:09:32.300 --> 00:09:35.165
そして、我々は、これらの予測を取る

141
00:09:35.165 --> 00:09:36.590
そして、我々はそれらを配置します

142
00:09:36.590 --> 00:09:41.989
空であった予測配列内の対応する位置。

143
00:09:41.989 --> 00:09:50.265
今、我々は常にこのローテーションを使用し
て、同じプロセスを繰り返すつもりです。

144
00:09:50.265 --> 00:09:54.375
なので、今はブルーを使うつもりですが、

145
00:09:54.375 --> 00:09:56.600
黄色、緑の部分、

146
00:09:56.600 --> 00:09:59.510
そして、我々は、モデルを作成し続ける

147
00:09:59.510 --> 00:10:04.317
そして、我々は予測のために赤い部分を維持
します。

148
00:10:04.317 --> 00:10:07.460
繰り返しますが、我々は、これらの予測を取
り、それを置く

149
00:10:07.460 --> 00:10:12.240
を予測配列の対応する部分に挿入します。

150
00:10:12.240 --> 00:10:19.244
そして、我々は再び黄色で、緑を繰り返しま
す。

151
00:10:19.244 --> 00:10:21.930
私が言及する必要がある何かは

152
00:10:21.930 --> 00:10:27.285
その K 倍は、日付としてシーケンシャル
である必要はありません。

153
00:10:27.285 --> 00:10:29.240
だから、それはシャッフルされていると思い
ます。

154
00:10:29.240 --> 00:10:33.480
私はそれをよりよく説明するためにこの方法
としてそれをした。

155
00:10:33.480 --> 00:10:36.255
しかし、我々が完了したら、我々は

156
00:10:36.255 --> 00:10:41.355
全体のトレーニングデータの予測を生成し、

157
00:10:41.355 --> 00:10:45.165
その後、我々は、全体のトレーニングデータ
を使用することができます

158
00:10:45.165 --> 00:10:50.210
1つの最後のモデルに適合し、テストデータ
のための今予測をするため。

159
00:10:50.210 --> 00:10:56.010
我々はこれを行うことができる別の方法は、
それぞれのためです

160
00:10:56.010 --> 00:11:02.470
我々は検証データの予測を行っていた4つの
モデルの一つ。

161
00:11:02.470 --> 00:11:03.720
同時に

162
00:11:03.720 --> 00:11:08.443
テストデータ全体について予測を行っている
可能性があります。

163
00:11:08.443 --> 00:11:10.230
そして、4つのモデルの後、

164
00:11:10.230 --> 00:11:12.463
私たちはちょうど最後に平均を取るでしょう
。

165
00:11:12.463 --> 00:11:15.275
テストの予測を4で割るだけです。

166
00:11:15.275 --> 00:11:17.430
しかし、それを行うには別の方法は、私が発
見した

167
00:11:17.430 --> 00:11:20.123
このように私は、ニューラルネットワークと
のより良い説明

168
00:11:20.123 --> 00:11:25.560
そして、あなたが全体のトレーニングデータ
を使用する方法

169
00:11:25.560 --> 00:11:31.570
ツリーベースのメソッドを使用したテストの
予測を生成します。

170
00:11:31.570 --> 00:11:34.570
だから、一度テストで予測を終えると、

171
00:11:34.570 --> 00:11:38.385
今度は別のモデルでもう一度始めることがで
きます。

172
00:11:38.385 --> 00:11:40.810
だから、空の予測を生成します

173
00:11:40.810 --> 00:11:43.710
あなたは、あなたの前のものの隣にそれを積
み重ねるでしょう。

174
00:11:43.710 --> 00:11:45.700
そして、あなたは同じプロセスを繰り返すで
しょう。

175
00:11:45.700 --> 00:11:48.265
あなたがしているまで、本質的にこれを繰り
返すでしょう

176
00:11:48.265 --> 00:11:51.785
同じレイヤーのすべてのモデルで終了します
。

177
00:11:51.785 --> 00:11:55.860
そして、これはあなたの新しいトレーニング
データセットになり、

178
00:11:55.860 --> 00:12:00.390
あなたが新しい層を持っている場合、一般的
に再びすべてを開始します。

179
00:12:00.390 --> 00:12:03.420
これは一般的に概念です。

180
00:12:03.420 --> 00:12:06.300
私たちはこれを言うことができるが、

181
00:12:06.300 --> 00:12:10.270
多くの層で拡張するためには、

182
00:12:10.270 --> 00:12:12.450
我々は、この
K-倍のパラダイムを使用します。

183
00:12:12.450 --> 00:12:18.880
しかし、通常、ニューラルネットワークは、
エポックのこの概念を持っています。

184
00:12:18.880 --> 00:12:25.222
我々は、我々は、ノード間の重みを再調整す
るのに役立つイテレーションを持っています
。

185
00:12:25.222 --> 00:12:28.270
ここでは、このオプションを持っていない

186
00:12:28.270 --> 00:12:30.100
スタッキングの方法です。

187
00:12:30.100 --> 00:12:33.010
しかし、我々は紹介することができます

188
00:12:33.010 --> 00:12:38.945
接続を介して初期データを再訪するこの機能
。

189
00:12:38.945 --> 00:12:44.400
したがって、ノードを接続する典型的な方法
は、我々が持っているものです

190
00:12:44.400 --> 00:12:49.375
すでにあなたがそれを入力ノードを持って探
検

191
00:12:49.375 --> 00:12:54.945
各ノードは、前のレイヤのノードと直接関連
しています。

192
00:12:54.945 --> 00:12:59.017
もう一つの方法は、これを行うには、と言う
ことです

193
00:12:59.017 --> 00:13:02.205
ノードが影響を受けるだけでなく、

194
00:13:02.205 --> 00:13:06.595
直接前の層のノードと接続されて、

195
00:13:06.595 --> 00:13:11.455
しかし、任意の前の層からすべての以前のノ
ードから。

196
00:13:11.455 --> 00:13:14.745
従って、これをよりよく説明するために、

197
00:13:14.745 --> 00:13:16.980
例を覚えていれば

198
00:13:16.980 --> 00:13:21.750
校長先生からの予言を使っていた

199
00:13:21.750 --> 00:13:27.594
彼は同時に学生からの予言をまた使用してい
ることができる。

200
00:13:27.594 --> 00:13:29.965
これは実際に非常によく働くことができる。

201
00:13:29.965 --> 00:13:33.645
また、初期データを改修することもできます
。

202
00:13:33.645 --> 00:13:35.140
だけでなく、予測、

203
00:13:35.140 --> 00:13:38.040
あなたは、実際に最初の x
データセットを置くことができます

204
00:13:38.040 --> 00:13:40.440
そして、あなたの予測に追加します。

205
00:13:40.440 --> 00:13:46.265
これは、多くのモデルを行っていない場合は
本当にうまく機能することができます。

206
00:13:46.265 --> 00:13:53.335
そのように、あなたは、最初のトレーニング
データを再訪する機会を得る

207
00:13:53.335 --> 00:13:55.870
そしてより多くの情報を捕獲することを試み
なさい。

208
00:13:55.870 --> 00:14:00.660
我々はすでに金属モデルが存在するので、

209
00:14:00.660 --> 00:14:05.610
モデルは、我々は新しい情報を探索すること
ができる場所に焦点を当てるしようとします
。

210
00:14:05.610 --> 00:14:08.690
だから、このような状況では、非常によく動
作します。

211
00:14:08.690 --> 00:14:13.980
また、これは、ターゲットエンコーディング
またはあなたが見た多くのエンコーディング
に非常に似ています

212
00:14:13.980 --> 00:14:19.975
データの一部を使用する前に、

213
00:14:19.975 --> 00:14:22.676
レッツは、カテゴリの列にコードを、と言う

214
00:14:22.676 --> 00:14:29.640
いくつかのクロス検証を考えると、ターゲッ
ト変数のいくつかの見積もりを生成します。

215
00:14:29.640 --> 00:14:33.720
次に、これをトレーニングデータに挿入しま
す。

216
00:14:33.720 --> 00:14:36.470
大丈夫、あなたはそれを積み重ねていない

217
00:14:36.470 --> 00:14:38.635
新しい列を作成しない場合と同様に、

218
00:14:38.635 --> 00:14:42.380
しかし、本質的には、ホールドアウトと1つ
の列を置き換える

219
00:14:42.380 --> 00:14:47.220
基本的に非常に類似しているあなたのターゲ
ット変数の予測。

220
00:14:47.220 --> 00:14:50.100
ターゲット変数のロジックを作成しました。

221
00:14:50.100 --> 00:14:55.000
そして、あなたは本質的にあなたのトレーニ
ングデータのアイデアに挿入されます。

