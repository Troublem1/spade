1
00:00:00.092 --> 00:00:04.002
音楽

2
00:00:04.002 --> 00:00:07.545
こんにちはみんな、このビデオでは、私はマ
トリックスのアプリケーションについてお話
します

3
00:00:07.545 --> 00:00:10.760
特徴抽出における因数分解法

4
00:00:10.760 --> 00:00:13.559
あなたは、アプローチのいくつかのアプリケ
ーションが表示されます

5
00:00:13.559 --> 00:00:16.500
特徴抽出と我々はそれを適用することができ
ます。

6
00:00:16.500 --> 00:00:19.853
私は実用的な詳細と一緒にいくつかの例を紹
介します。

7
00:00:19.853 --> 00:00:22.900
ここで推奨事項の典型的な例です。

8
00:00:22.900 --> 00:00:27.370
我々は、年齢、地域、関心のようなユーザー
に関するいくつかの情報を持っていると仮定

9
00:00:27.370 --> 00:00:29.990
性別、年の長さのような項目。

10
00:00:29.990 --> 00:00:33.390
また、我々は、ユーザーがいくつかの項目に
与えた評価を知っている。

11
00:00:33.390 --> 00:00:37.940
これらの評価は、対応する行を持つユーザー
項目マトリックスにまとめることができます
。

12
00:00:37.940 --> 00:00:42.140
図に示すように、ユーザー、および項目に対
応する列。

13
00:00:42.140 --> 00:00:45.917
座標 i, j を有するセルにおいて、ユ
ーザまたはエージェントがセレクタ i、

14
00:00:45.917 --> 00:00:47.590
項目 j を与える。

15
00:00:47.590 --> 00:00:51.210
ユーザーがいくつかの機能 Ui
を持っていると仮定します。

16
00:00:51.210 --> 00:00:54.460
jth アイテムは、Mj
の機能に対応しています。

17
00:00:54.460 --> 00:00:57.870
そしてこれらの特徴のスカラープロダクトは
評価の Rij を作り出す。

18
00:00:57.870 --> 00:01:02.580
ここでは、行列分解を適用してこれらの機能
を学習します。

19
00:01:02.580 --> 00:01:04.260
アイテムとユーザー。

20
00:01:04.260 --> 00:01:07.520
時々これらの特徴は解釈を有することができ
る。

21
00:01:07.520 --> 00:01:12.076
項目の最初の特徴のようにまたは類似した何
かの測定することができる。

22
00:01:12.076 --> 00:01:16.470
しかし、一般的には、我々が使用できるいく
つかの余分な機能として、それらを考慮する
必要があります

23
00:01:16.470 --> 00:01:21.350
私たちが前にしたのと同じようにユーザーを
符号化するために、コーダーまたはコーダを
ラベリング。

24
00:01:21.350 --> 00:01:25.300
具体的には、製品の規模に関する当社の前提
は以下のとおりです。

25
00:01:25.300 --> 00:01:27.480
我々は、ユーザーのすべての属性を提示する
場合

26
00:01:27.480 --> 00:01:33.520
行列としての項目は、マトリックスの製品は
非常にマトリックスの評価に近いされます。

27
00:01:33.520 --> 00:01:36.100
言い換えれば、どのように行列のあなたとを
見つけるために

28
00:01:36.100 --> 00:01:39.820
M
は、そのような彼らの製品は、マトリックス
R を与える。

29
00:01:39.820 --> 00:01:44.210
この方法では、このアプローチは行列分解ま
たは行列構成と呼ばれます。

30
00:01:44.210 --> 00:01:47.920
前の例では、行と列に関連する機能の両方を
使用しました。

31
00:01:47.920 --> 00:01:50.920
しかし、機能が行に対応していない場合もあ
ります。

32
00:01:50.920 --> 00:01:52.640
別の例を考えてみましょう。

33
00:01:52.640 --> 00:01:56.908
私たちがテキストであると仮定して、あなた
はどのように通常の文字列を分類覚えていま
すか?

34
00:01:56.908 --> 00:02:02.780
我々は、特徴を抽出し、各文書は、大規模な
疎原子炉によって記述された。

35
00:02:02.780 --> 00:02:06.200
これらの解析機能に対して行列分解を行うと
、

36
00:02:06.200 --> 00:02:11.070
黄色で表示されるインデックスの表現、およ
び緑で表示される用語。

37
00:02:11.070 --> 00:02:12.940
我々は何らかの方法で表現を使用することが
できますが

38
00:02:12.940 --> 00:02:16.560
ジャンプ, 我々は犬のための表現にのみ興
味を持っている.

39
00:02:16.560 --> 00:02:19.770
今、すべての文書は、小型高密度リアクター
によって記述されています。

40
00:02:19.770 --> 00:02:24.320
これらは私たちの特徴であり、我々は以前の
例と同様の方法でそれらを使用することがで
きます。

41
00:02:24.320 --> 00:02:27.820
このケースは、多くの場合、ディメンション
のエネルギー削減と呼ばれます。

42
00:02:27.820 --> 00:02:31.200
これは、機能メトリックのサイズを減らすた
めに非常に効率的な方法です, と

43
00:02:31.200 --> 00:02:35.340
カテゴリのものから実際の価値のある機能を
抽出します。

44
00:02:35.340 --> 00:02:38.990
大会では、多くの場合、購入のためのさまざ
まなオプションがあります。

45
00:02:38.990 --> 00:02:47.540
たとえば、テキストデータを使用して、大き
な ram
などのバックを実行することができます。

46
00:02:47.540 --> 00:02:49.180
行列最適化手法を用いて,

47
00:02:49.180 --> 00:02:52.550
これらのすべての行列からフィーチャを抽出
することができます。

48
00:02:52.550 --> 00:02:56.270
結果として得られる行列は小さくなるため、
簡単に結合でき、

49
00:02:56.270 --> 00:02:59.370
ツリーベースのモデルでは、機能の一体性を
使用します。

50
00:02:59.370 --> 00:03:03.110
今私は行列分解についていくつかのコメント
をしたい。

51
00:03:03.110 --> 00:03:06.440
だけでなく、我々は全体の行列を減らすため
に制約されていません

52
00:03:06.440 --> 00:03:11.020
因数分解を列のサブセットに適用し、そのま
ま他の部分をそのまま残すことができます。

53
00:03:11.020 --> 00:03:13.530
削減に加えて、圧力ボードを使用することが
できます

54
00:03:13.530 --> 00:03:16.160
同じデータの別のプレゼンテーションを取得
する。

55
00:03:17.250 --> 00:03:18.580
これは特に便利です。

56
00:03:18.580 --> 00:03:24.360
それはモデルの速度を提供し、より良いにつ
ながるので、例。

57
00:03:24.360 --> 00:03:27.440
もちろん行列分解は変換の損失であり、

58
00:03:27.440 --> 00:03:30.650
言い換えれば、我々は、検索の削減後にいく
つかの情報を失うことになる。

59
00:03:31.670 --> 00:03:35.300
このアプローチの効率性は、特定のタスクに
大きく依存し、

60
00:03:35.300 --> 00:03:37.650
潜在的な要因の数を選択します。

61
00:03:37.650 --> 00:03:41.730
数値はハイパーパラメータとして考慮する必
要があり、調整する必要があります。

62
00:03:41.730 --> 00:03:45.834
これは、5と100の間の要因の数を選択す
ることをお勧めします。

63
00:03:45.834 --> 00:03:49.550
次に、一般的な考え方から特定の実装に切り
替えてみましょう。

64
00:03:49.550 --> 00:03:53.608
いくつかの行列分解法が回路に実装されてい
る

65
00:03:53.608 --> 00:03:56.580
最も有名な分解および PCA として。

66
00:03:56.580 --> 00:03:58.780
さらに、それらの使用は
TruncatedSVD を含んでいた、

67
00:03:58.780 --> 00:04:01.010
疎行列を操作することができます.

68
00:04:01.010 --> 00:04:04.530
たとえば、テキストデータセットの場合は非
常に便利です。

69
00:04:04.530 --> 00:04:10.180
また、いわゆる非負行列分解、または
NMF が存在します。

70
00:04:10.180 --> 00:04:14.333
それはすべての隠された要因が非否定的であ
るという追加の制限を課す

71
00:04:14.333 --> 00:04:17.520
これは、0または正の数値のいずれかです。

72
00:04:17.520 --> 00:04:19.990
これは、非負のマトリックスにのみ適用する
ことができます。

73
00:04:19.990 --> 00:04:25.370
例えば、行列は、文書内の各単語のすべての
表現の出現。

74
00:04:25.370 --> 00:04:27.480
NMF には興味深いプロパティがあり、

75
00:04:27.480 --> 00:04:32.490
データは、デシジョンツリーに適したデータ
になるように変換されます。

76
00:04:32.490 --> 00:04:35.500
マイクロソフトモバイル分類の課題から画像
を見てみましょう。

77
00:04:35.500 --> 00:04:41.460
NMF は、データが軸に平行な線を変換す
ることがわかる。

78
00:04:41.460 --> 00:04:44.080
行列の分解に関するさらにいくつかのメモ。

79
00:04:44.080 --> 00:04:47.140
基本的にそれらは線形モデルに非常に類似し
ている、従って

80
00:04:47.140 --> 00:04:51.250
我々は、線形モデルに使用するのと同じ変換
のトリックを使用することができます。

81
00:04:51.250 --> 00:04:53.480
だから、標準の NMF に加えて、

82
00:04:53.480 --> 00:04:57.810
データを変換するために因数分解を適用する
ことをお勧めします。

83
00:04:57.810 --> 00:05:00.700
ここでは、競争から別のプロットです。

84
00:05:00.700 --> 00:05:04.760
この2つの変換によって異なる機能が生成さ
れることは明らかですが、

85
00:05:04.760 --> 00:05:06.610
私たちは最高のものを選択する必要はありま
せん。

86
00:05:06.610 --> 00:05:09.680
その代わりに、両方を使用することが有益で
す。

87
00:05:09.680 --> 00:05:14.130
行列分解は trainable
変換であることに注意したい

88
00:05:14.130 --> 00:05:16.322
独自のパラメータを持ちます。

89
00:05:16.322 --> 00:05:18.990
だから我々は注意する必要があります,
と同じ変換を使用する

90
00:05:18.990 --> 00:05:20.360
データセットのすべての部分。

91
00:05:21.360 --> 00:05:24.630
各部分を個別に読み取って変換するのは間違
っていますが、

92
00:05:24.630 --> 00:05:28.020
その場合には、2つの異なる変換を取得しま
す。

93
00:05:28.020 --> 00:05:30.490
これは、見つけるのは難しいだろうエラーに
つながることができます。

94
00:05:31.630 --> 00:05:34.480
正しい方法を以下に示します。

95
00:05:34.480 --> 00:05:40.030
すべてのデータのデータ情報は、唯一の個々
の作品に適用されます。

96
00:05:40.030 --> 00:05:44.550
要約すると、行列の構成は、寸法の削減に非
常に一般的なアプローチです。

97
00:05:44.550 --> 00:05:46.390
と特徴抽出。

98
00:05:46.390 --> 00:05:49.410
これは、実際のものにカテゴリ機能を変換す
るために使用することができます。

99
00:05:50.730 --> 00:05:55.440
線形モデルのためのトリックはまたマトリッ
クスの分解のために適している。

100
00:05:55.440 --> 00:05:56.413
ご注意をありがとうございました。

101
00:05:56.413 --> 00:06:03.598
音楽

102
00:06:03.598 --> 00:06:09.314
音

