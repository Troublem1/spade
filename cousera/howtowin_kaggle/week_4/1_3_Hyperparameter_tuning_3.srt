1
00:00:00.000 --> 00:00:03.780
音楽

2
00:00:03.780 --> 00:00:08.272
このビデオでは、簡単に説明しますニューラ
ルネットワークライブラリと

3
00:00:08.272 --> 00:00:13.979
次に、ニューラルネットワークと線形モデル
のハイパーパラメータをチューニングする方
法について説明します。

4
00:00:13.979 --> 00:00:20.966
多くのフレームワーク、Keras、Ten
sorFlow、MxNet、PyTorc
h があります。

5
00:00:20.966 --> 00:00:23.009
選択は本当に個人的ですが、

6
00:00:23.009 --> 00:00:29.100
すべてのフレームワークは、競合タスクに十
分な機能を実装しています。

7
00:00:29.100 --> 00:00:34.360
Keras は確かに Kaggle で最
も人気があり、非常にシンプルなインターフ
ェイスを持っています。

8
00:00:35.650 --> 00:00:40.070
Keras を使用してネットワークをトレ
ーニングするには、数十行しかかかりません
。

9
00:00:41.570 --> 00:00:46.000
TensorFlow は生産のために企業
によって広く利用されている。

10
00:00:46.000 --> 00:00:50.320
そして PyTorch は深い学習の研究
コミュニティで非常に普及している。

11
00:00:51.410 --> 00:00:54.230
私は個人的に PyTorch
を試してみることをお勧め

12
00:00:54.230 --> 00:00:58.690
Keras は、最も透明で使いやすいフレ
ームワークです。

13
00:00:59.730 --> 00:01:03.810
さて、どのようにネットワークでハイパーパ
ラメータをチューニングするのですか?

14
00:01:03.810 --> 00:01:07.250
今は密なニューラルネットワークについて話
します

15
00:01:07.250 --> 00:01:11.280
これは、完全に接続されたレイヤのみで構成
されるネットワークです。

16
00:01:12.430 --> 00:01:15.428
3層のニューラルネットワークから始めまし
ょう

17
00:01:15.428 --> 00:01:20.920
我々は1層あたりのニューロンの数を増やす
場合、私たちは何が起こることを期待するの
ですか?

18
00:01:22.170 --> 00:01:25.860
ネットワークは今、より複雑な意思決定の境
界を学ぶことができます

19
00:01:25.860 --> 00:01:27.602
従ってそれはより速く合う。

20
00:01:28.870 --> 00:01:34.100
レイヤーの数が増えると同じことが起こるは
ずですが、

21
00:01:34.100 --> 00:01:39.620
最適化の問題のために、学習も収束に停止す
ることができます。

22
00:01:40.680 --> 00:01:44.700
しかし、とにかく、あなたのネットワークが
十分に強力ではないと思う場合は、

23
00:01:44.700 --> 00:01:47.760
あなたは別のレイヤーを追加し、何が起こる
かを試すことができます。

24
00:01:48.940 --> 00:01:52.666
私のお勧めはここでは非常に単純な何かを開
始することです

25
00:01:52.666 --> 00:01:58.640
1層または2層、64単位をレイヤーごとに
言う。

26
00:01:58.640 --> 00:02:02.330
コードをデバッグし、トレーニングと
[聞こえない]
損失が下がることを確認します。

27
00:02:03.510 --> 00:02:08.660
その後、トレーニングセットを
overfit
ことができる構成を探してみて、

28
00:02:09.830 --> 00:02:11.280
別の正気のチェックと同じように。

29
00:02:13.120 --> 00:02:16.550
それ以降は、ネットワークで何かを調整する
時間です。

30
00:02:18.140 --> 00:02:22.780
ニューラルネットワークの重要な部分の一つ
は、選択された最適化方法です。

31
00:02:23.890 --> 00:02:28.560
大まかに言えば、我々はどちらかのバニラ確
率勾配降下を選ぶことができます

32
00:02:28.560 --> 00:02:34.040
運動量やアダムのような近代的な適応方法の
一つは、

33
00:02:34.040 --> 00:02:36.680
Adadelta、Adagrad など。

34
00:02:38.150 --> 00:02:42.138
このスライドでは、アダプティブメソッドは
緑で色分けされており、

35
00:02:42.138 --> 00:02:43.904
赤で SGD に比べて。

36
00:02:43.904 --> 00:02:47.869
私はここでは、アダプティブメソッドは本当
にできることを示したい

37
00:02:47.869 --> 00:02:50.740
より速くトレーニングセットに合わせます。

38
00:02:50.740 --> 00:02:54.908
しかし、私の経験では、彼らはまた、オーバ
ーフィット回避につながる。

39
00:02:54.908 --> 00:02:58.969
明白な古い確率勾配降下はより遅い収束する
が、

40
00:02:58.969 --> 00:03:02.270
訓練されたネットワークは通常よりよく一般
化。

41
00:03:03.730 --> 00:03:05.110
アダプティブメソッドは便利ですが、

42
00:03:05.110 --> 00:03:09.100
しかし、設定の他の分類と回帰。

43
00:03:11.090 --> 00:03:13.960
今ここにあなたのための質問です。

44
00:03:13.960 --> 00:03:15.570
ただサイズを維持します。

45
00:03:15.570 --> 00:03:18.780
バッチサイズを大きくすると、何を期待すべ
きか

46
00:03:18.780 --> 00:03:20.480
その他ハイパーパラメータ固定?

47
00:03:21.800 --> 00:03:26.730
実際には、それは巨大なバッチサイズがより
多くのオーバーフィット回避につながること
が判明した。

48
00:03:26.730 --> 00:03:31.110
500オブジェクトのバッチは、経験では大
きいと言う。

49
00:03:32.290 --> 00:03:37.664
私は32または64の周りの値を選択するこ
とをお勧めします。

50
00:03:37.664 --> 00:03:40.931
その後、ネットワークを参照してください場
合は、まだオーバーフィット回避を減少しよ
うとしている

51
00:03:40.931 --> 00:03:41.800
バッチサイズ。

52
00:03:43.220 --> 00:03:45.220
underfitting
なら、増やしてみては。

53
00:03:46.420 --> 00:03:50.220
送信トレイの数が固定されていることを知っ
ている、

54
00:03:50.220 --> 00:03:55.050
その後、バッチサイズが2の要因によって減
少したネットワーク

55
00:03:55.050 --> 00:03:59.710
元のネットワークと比較して2回以上更新さ
れます。

56
00:04:00.840 --> 00:04:03.120
だから考慮してこれを取る。

57
00:04:03.120 --> 00:04:07.290
多分あなたは、ネットワークの数を減らすか
、少なくとも何とかそれを調整する必要があ
ります。

58
00:04:09.110 --> 00:04:14.490
バッチサイズも小さすぎることはありません
, グラデーションはあまりにも騒々しいさ
れます.

59
00:04:15.590 --> 00:04:20.620
グラデーションのブーストと同じように、適
切な学習率を設定する必要があります。

60
00:04:20.620 --> 00:04:24.270
学習率が高すぎると、ネットワークが収束せ
ず、

61
00:04:24.270 --> 00:04:28.610
あまりにも小さな学習率では、ネットワーク
は永遠に学ぶことができます。

62
00:04:30.060 --> 00:04:33.140
学習率は高すぎず、低すぎないようにする必
要があります。

63
00:04:33.140 --> 00:04:37.160
そのため、最適な学習率は他のパラメータに
よって異なります。

64
00:04:38.160 --> 00:04:43.750
私は通常、巨大な学習率を開始し、0.1
と言う、それを下げることを試みるまで

65
00:04:43.750 --> 00:04:50.470
私は、ネットワークが収束し、私はさらに修
正しようとすると1つを見つける。

66
00:04:50.470 --> 00:04:53.900
興味深いことに、バッチサイズとの間の接続
がある

67
00:04:53.900 --> 00:04:55.460
学習率。

68
00:04:55.460 --> 00:05:00.450
理論的には特定のタイプのモデルに接地され
ていますが、

69
00:05:00.450 --> 00:05:05.330
人々は通常、それを使用しても、実際にいく
つかの人々としてそれを使用する

70
00:05:05.330 --> 00:05:08.610
ニューラルネットワークの経験則。

71
00:05:08.610 --> 00:05:10.670
接続は次のとおりです。

72
00:05:10.670 --> 00:05:14.420
アルファ係数でバッチサイズを大きくすると
、

73
00:05:14.420 --> 00:05:18.640
また、同じ要因によって学習率を高めること
ができます。

74
00:05:19.890 --> 00:05:22.710
しかし、大きなバッチサイズを覚えて、

75
00:05:24.630 --> 00:05:26.970
ネットワークのオーバーフィット回避が発生
しやすくなります。

76
00:05:26.970 --> 00:05:29.840
だから、ここで良い正則化が必要です。

77
00:05:31.092 --> 00:05:37.350
いつか前に、人々は主に重みの L2 と
L1 正則化を使用します。

78
00:05:37.350 --> 00:05:41.280
最近では、ほとんどの人がドロップアウト正
規化を使用します。

79
00:05:41.280 --> 00:05:47.257
したがって、ネットワークオーバーフィット
回避が表示される場合は、まず、ドロップア
ウト層を試してみてください。

80
00:05:47.257 --> 00:05:53.310
ドロップアウトの確率と、ドロップアウトレ
イヤを挿入する場所を上書きすることができ
ます。

81
00:05:53.310 --> 00:05:58.010
通常、人々は、ネットワークの終わりに近い
ドロップアウト層を追加すると、

82
00:05:58.010 --> 00:06:02.400
しかし、それはすべての層にいくつかのドロ
ップアウトを追加しても大丈夫だ、それも動
作します。

83
00:06:03.870 --> 00:06:09.230
ドロップアウトは、ネットワークが本当に重
要な機能を見つけるために、何が働いたこと
ができます

84
00:06:09.230 --> 00:06:14.800
私にとっては、データ層の直後に、非常に最
初の層としてドロップアウトを持つことです
。

85
00:06:15.920 --> 00:06:19.850
このようにいくつかの情報は、非常に最初に
完全に失われる

86
00:06:19.850 --> 00:06:23.070
ネットワークのパフォーマンス低下を観察し
ます。

87
00:06:24.990 --> 00:06:28.360
私達が使用した興味深い正則化の技術

88
00:06:28.360 --> 00:06:32.700
我々はそれを呼び出すように [不明]
競争では、静的な dropconnect
です。

89
00:06:34.242 --> 00:06:41.920
だから、通常我々は、入力層を密に接続して
、128単位と言うことを思い出してくださ
い。

90
00:06:41.920 --> 00:06:47.048
我々は、代わりに、非常に最初の非表示の層
を使用します。

91
00:06:47.048 --> 00:06:51.820
ユニットの膨大な数は、4096台と言う。

92
00:06:53.430 --> 00:06:58.860
これは、通常の競争のための巨大なネットワ
ークであり、それがひどく食べさせ過ぎるさ
れます。

93
00:07:00.520 --> 00:07:05.110
しかし、今それを regularlize
に、我々はランダムにドロップ 99% の

94
00:07:05.110 --> 00:07:10.270
入力レイヤと最初の非表示レイヤ間の接続。

95
00:07:11.700 --> 00:07:16.310
もともと dropconnect では静
dropconnect と呼んでいる

96
00:07:16.310 --> 00:07:23.180
すべての学習イテレーションでランダム接続
を削除する必要がありますが、

97
00:07:23.180 --> 00:07:28.657
学習プロセス全体のネットワークの接続パタ
ーンを修正します。

98
00:07:28.657 --> 00:07:33.057
だから、ポイントを参照してください、我々
は隠しユニットの数を増やすが、

99
00:07:33.057 --> 00:07:37.550
最初の非表示層のパラメータの数は小さいま
まです。

100
00:07:39.270 --> 00:07:43.700
第2層の重み行列は、とにかく巨大になるこ
とに注意してください,

101
00:07:43.700 --> 00:07:46.550
しかし、それは練習で大丈夫であることが判
明した。

102
00:07:48.590 --> 00:07:51.600
これは非常に強力な
regularizations です。

103
00:07:51.600 --> 00:07:55.840
また、接続パターンが異なるネットワークの
多くは、

104
00:07:55.840 --> 00:07:59.625
静的な dropconnect
のないネットワークよりずっとよい。

105
00:08:00.850 --> 00:08:06.040
すべての権利は、モデルの最後のクラスを議
論する私の神経モデルです。

106
00:08:07.590 --> 00:08:13.000
まだ、注意深く調整された生きている
GPM
はおそらくサポートベクトル機械を打つ

107
00:08:13.000 --> 00:08:16.100
大規模なスパースデータセットでも。

108
00:08:16.100 --> 00:08:21.480
SVM のは、本当に有益であるほとんどす
べてのチューニングを必要としません。

109
00:08:22.650 --> 00:08:27.788
SVM の分類と回帰のための SK
の学習者または実装されています

110
00:08:27.788 --> 00:08:32.534
libLinear と libSVM と
呼ばれるライブラリからのアルゴリズムへの
ラッパー。

111
00:08:33.570 --> 00:08:38.677
libLinear と libSVM の
最新バージョンは、マルチコア大会をサポー
トしています,

112
00:08:38.677 --> 00:08:43.549
しかし、残念ながら Sklearn でマ
ルチコアバージョンを使用することはできま
せんが、

113
00:08:43.549 --> 00:08:47.900
したがって、このオプションを使用するには
、これらのライブラリを手動でコンパイルす
る必要があります。

114
00:08:49.770 --> 00:08:53.340
そして、私は誰も最近、カーネル SVC
を使用してきたことがない

115
00:08:53.340 --> 00:08:56.750
したがって、このビデオでは、線形 SVM
についてのみ話します。

116
00:08:58.250 --> 00:09:03.758
Sklearn では、様々なロジスティッ
クと線形回帰を見つけることもできます。

117
00:09:03.758 --> 00:09:09.462
正則化オプションと、また、あなたの
declassifier
とリグレッサとして。

118
00:09:09.462 --> 00:09:12.050
我々はすでにそれらを言及しながらメトリッ
クを議論した。

119
00:09:13.160 --> 00:09:18.980
メモリに収まらないデータセットの場合は、
Vowpal Wabbit
を使用できます。

120
00:09:18.980 --> 00:09:23.380
これは、オンラインファッションの線形モデ
ルの学習を実装しています。

121
00:09:23.380 --> 00:09:27.680
これは、ハードドライブから直接行でデータ
行を読み取り、

122
00:09:27.680 --> 00:09:30.958
メモリ内のデータセット全体を読み込むこと
はありません。

123
00:09:30.958 --> 00:09:35.900
したがって、非常に巨大なデータセットで学
ぶことができます。

124
00:09:37.280 --> 00:09:43.040
フローと呼ばれる線形モデルのためのオンラ
イン学習の一手法正規化リーダー

125
00:09:43.040 --> 00:09:50.100
または短い FTRL
は、特にいくつかの時間前に人気があった。

126
00:09:50.100 --> 00:09:52.940
それは Vowpal Wabbit
で実装されていますが、

127
00:09:52.940 --> 00:09:56.580
純粋な Python
での実装もたくさんあります。

128
00:09:57.772 --> 00:10:02.640
我々は通常、線形モデルを調整する必要があ
るハイパーパラメータは L2 と

129
00:10:02.640 --> 00:10:04.600
重みの L1 正則化。

130
00:10:05.720 --> 00:10:09.380
もう一度、正則化はより高いので赤い色と示
される

131
00:10:09.380 --> 00:10:13.900
正則化の重みは、より多くのモデル闘争は何
かを学ぶことです。

132
00:10:14.970 --> 00:10:20.080
しかし、それを知っている,
パラメータは、SVM
で参照してくださいに反比例している

133
00:10:20.080 --> 00:10:23.270
正規化の重みなので、ダイナミクスは反対で
す。

134
00:10:25.030 --> 00:10:29.850
実際には、我々の場合のパラメータの意味に
ついて考える必要はありません

135
00:10:29.850 --> 00:10:31.700
1つのパラメータでしょ?

136
00:10:31.700 --> 00:10:36.290
我々は、いくつかの値を試してみて、最高の
作品を見つける。

137
00:10:37.530 --> 00:10:44.065
SVM については、私は通常、非常に小さ
な種子を起動すると、マイナス6の力を10
と言う

138
00:10:44.065 --> 00:10:49.530
それから私はそれを増加しようとすると、1
0の要因によって、毎回乗算します。

139
00:10:50.770 --> 00:10:55.240
私は小さい値から始まるパラメータ C
が大きいので、

140
00:10:55.240 --> 00:10:56.890
トレーニングにかかる時間が長くなります。

141
00:10:58.170 --> 00:11:02.070
どのタイプの正則化、L1 または L2
を選択しますか?

142
00:11:03.980 --> 00:11:06.380
実際に、私の答えは両方を試すことです。

143
00:11:06.380 --> 00:11:12.070
私の心には、実際にはかなり似ていると
L1
は私たちを与えることができる1つの利点

144
00:11:12.070 --> 00:11:18.020
はウェイトスパースなので、スパースパター
ンをフィーチャー選択に使用することができ
ます。

145
00:11:19.360 --> 00:11:23.180
一般的な私はここにあまりに多くの時間を費
やすことはありません与えたいアドバイス

146
00:11:23.180 --> 00:11:27.700
チューニングハイパーパラメータ、特に競争
が始まったばかりです。

147
00:11:28.840 --> 00:11:32.280
パラメータをチューニングすることで競争に
勝つことはできません。

148
00:11:32.280 --> 00:11:36.780
適切な機能、ハッキング、リーク、洞察力は

149
00:11:36.780 --> 00:11:41.409
既定の機能に基づいて、慎重にチューニング
されたモデルよりもはるかに多くを提供しま
す。

150
00:11:42.590 --> 00:11:44.350
私も我慢してアドバイスします。

151
00:11:45.690 --> 00:11:47.640
何度か私個人のミスでした。

152
00:11:47.640 --> 00:11:53.271
私は、トレーニングモデルに10分以上を費
やすのが嫌いだったと驚いた

153
00:11:53.271 --> 00:11:59.199
私はそれが長い時間のために訓練させていた
だければどのくらいのモデルが向上する可能
性があります。

154
00:11:59.199 --> 00:12:02.262
そして最後に、すべての平均。

155
00:12:02.262 --> 00:12:06.516
提出するときは、別のランダムから始まる5
つのモデルを学ぶ

156
00:12:06.516 --> 00:12:09.680
予測を初期化および平均します。

157
00:12:11.150 --> 00:12:16.579
それは多くの実際に役立ち、何人かの人々平
均だけ無作為の種、

158
00:12:16.579 --> 00:12:20.330
また、最適な値の周りの他のパラメータ。

159
00:12:20.330 --> 00:12:24.920
たとえば、エクストラブーストに最適な深度
が5の場合、

160
00:12:24.920 --> 00:12:29.819
我々は、深さ3、4、および5で平均 3
digiboosts
することができます。

161
00:12:29.819 --> 00:12:32.881
うわー、それは我々ができれば良いでしょう

162
00:12:32.881 --> 00:12:37.534
深さ4、5、6で平均 3
digiboosts。

163
00:12:38.660 --> 00:12:40.570
最後に、この講義では、

164
00:12:40.570 --> 00:12:46.300
ハイパーパラメータ最適化のための一般的な
パイプラインとは何かを説明しました。

165
00:12:46.300 --> 00:12:50.890
特に重要なハイパーパラメータが何を得るか
を見ました

166
00:12:50.890 --> 00:12:54.900
いくつかのモデル,
グラデーションブーストデシジョンツリー,

167
00:12:54.900 --> 00:12:59.630
ランダムフォレストと余分な木、ニューラル
ネットワーク、および線形モデル。

168
00:13:00.870 --> 00:13:04.985
私はあなたがこの講義で面白いものを見つけ
て、後であなたを参照してください願ってい
ます。

169
00:13:04.985 --> 00:13:14.985
音楽

