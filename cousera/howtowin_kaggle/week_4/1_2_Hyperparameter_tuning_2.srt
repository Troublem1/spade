1
00:00:00.000 --> 00:00:03.876
音楽

2
00:00:03.876 --> 00:00:09.178
このビデオでは、我々はハイパーパラメータ
の最適化についてお話します

3
00:00:09.178 --> 00:00:11.161
いくつかのツリーベースのモデル。

4
00:00:11.161 --> 00:00:15.707
今日では、XGBoost と
LightGBM
は本当に金本位になった。

5
00:00:15.707 --> 00:00:20.325
それらは非常に多目的な勾配のちょうど素晴
らしい実施である後押しした

6
00:00:20.325 --> 00:00:22.790
デシジョンツリーモデル。

7
00:00:22.790 --> 00:00:27.580
私たちがいた時に正確に現れた
CatBoost ライブラリもあります。

8
00:00:27.580 --> 00:00:33.350
このコースを準備しているので、CatBo
ost は人々の心を獲得する時間がありま
せんでした。

9
00:00:33.350 --> 00:00:38.105
しかし、それは非常に興味深いと有望に見え
るので、それをチェックアウト。

10
00:00:38.105 --> 00:00:40.988
RandomForest
の非常に素晴らしい実装があり、

11
00:00:40.988 --> 00:00:42.530
ExtraTrees モデル
sklearn.

12
00:00:42.530 --> 00:00:48.630
これらのモデルは強力であり、勾配ブースト
と一緒に使用することができます。

13
00:00:49.940 --> 00:00:54.940
そして最後に、正規化貪欲フォレストと呼ば
れるモデルがあります。

14
00:00:54.940 --> 00:00:59.390
これは、いくつかの大会から非常に素晴らし
い結果を示したが、その実装は

15
00:00:59.390 --> 00:01:04.420
非常に遅いと使用するのは難しいが、小さな
データセットでそれを試すことができます。

16
00:01:05.450 --> 00:01:10.030
さて、XGBoost と
LightGBM
で重要なパラメータは何ですか?

17
00:01:11.500 --> 00:01:17.960
2つのライブラリには同様のパラメータがあ
り、XGBoost の名前を使用します。

18
00:01:17.960 --> 00:01:21.991
そして、スライドの右半分には、何とか大ま
かに表示されます

19
00:01:21.991 --> 00:01:25.242
LightGBM
からの対応するパラメータ名。

20
00:01:25.242 --> 00:01:30.524
パラメータを理解するために、我々はよりよ
く理解する方法 XGBoost と

21
00:01:30.524 --> 00:01:33.720
LightGBM は、少なくとも非常に高
いレベルで動作します。

22
00:01:35.280 --> 00:01:40.033
これらのモデルは何をすべきか、これらのモ
デルは、次々とデシジョンツリーを構築する

23
00:01:40.033 --> 00:01:42.760
特定の目的を徐々に最適化する。

24
00:01:44.270 --> 00:01:48.405
そして、まず、ツリーの構築プロセスを制御
する多くのパラメータがあります。

25
00:01:48.405 --> 00:01:52.200
Max_depth
はツリーの最大深度です。

26
00:01:53.310 --> 00:01:57.770
そしてもちろん、より深い木は、それがデー
タセットに合うことができるより良い成長す
ることができます。

27
00:01:58.820 --> 00:02:02.850
このパラメータを増やすと、列車セットへの
フィッティングが速くなります。

28
00:02:03.970 --> 00:02:08.408
タスクに応じて、最適な深さは、多くの異な
ることができます

29
00:02:08.408 --> 00:02:11.748
時には2、時には27です。

30
00:02:11.748 --> 00:02:17.073
奥行きを増やせば overfit にモデ
ルを手に入れることができず、つまりモデル

31
00:02:17.073 --> 00:02:22.260
は、深度を大きくするにつれて、検証セット
の方が良くなってきています。

32
00:02:23.330 --> 00:02:27.920
それは重要な相互作用の多くがあるという記
号である場合もある

33
00:02:27.920 --> 00:02:29.480
データから抽出します。

34
00:02:29.480 --> 00:02:33.410
だから、チューニングを停止し、いくつかの
機能を生成しようとする方が良い。

35
00:02:34.940 --> 00:02:38.560
私は約7の max_depth
から始めることをお勧めします。

36
00:02:40.320 --> 00:02:44.170
また、深さを増やすように覚えて、

37
00:02:44.170 --> 00:02:46.090
学習には長い時間がかかります。

38
00:02:46.090 --> 00:02:49.091
ので、非常に高い値に深さを設定しないでく
ださい

39
00:02:49.091 --> 00:02:52.196
あなたがそれを必要とする 100%
確信していない限り。

40
00:02:52.196 --> 00:02:56.120
LightGBM
では、数を制御することが可能です。

41
00:02:56.120 --> 00:02:59.240
最大深度ではなく、ツリーに残します。

42
00:03:00.290 --> 00:03:03.590
それは、結果の木は非常に深いことができる
ので、うれしいです

43
00:03:03.590 --> 00:03:08.150
しかし、葉の数が少なく、フィット以上では
ありません。

44
00:03:08.150 --> 00:03:13.060
いくつかの単純なパラメータは、ツリーを供
給するときに使用するオブジェクトの一部を
制御します。

45
00:03:14.380 --> 00:03:16.210
0から1までの値です。

46
00:03:17.760 --> 00:03:23.130
一つは、それが良い常にすべてのオブジェク
トを使用していると思うかもしれない、右?

47
00:03:23.130 --> 00:03:25.860
しかし、実際には、それがないことが判明し
た。

48
00:03:27.550 --> 00:03:31.350
実際には、すべての期間で使用されているオ
ブジェクトのほんの一部である場合、

49
00:03:31.350 --> 00:03:35.400
その後、モデルはオーバーフィット回避にな
りにくいです。

50
00:03:35.400 --> 00:03:41.740
したがって、オブジェクトの一部を使用して
、モデルは、列車のセットで遅く収まるが、
で

51
00:03:41.740 --> 00:03:48.640
同じ時間は、おそらくこのオーバーフィット
モデルよりも一般化されます。

52
00:03:48.640 --> 00:03:52.060
従って、それは一種の正則化として働く。

53
00:03:53.390 --> 00:03:58.771
同様に、我々は機能のほんの一部を考慮する
ことができる場合 [聞こえない] 分割、

54
00:03:58.771 --> 00:04:04.430
これは、パラメータ
colsample_bytree と
colsample_bylevel
によって制御されます。

55
00:04:04.430 --> 00:04:07.310
もう一度、モデルがフィッティングを超えて
いる場合、

56
00:04:07.310 --> 00:04:10.650
これらのパラメータを下げようとすることが
できます。

57
00:04:10.650 --> 00:04:15.040
また、様々な正則化パラメータ、min_c
hild_weight、

58
00:04:15.040 --> 00:04:17.410
ラムダ、アルファ、その他。

59
00:04:18.410 --> 00:04:20.895
一番大事なのは
min_child_weight です。

60
00:04:22.400 --> 00:04:26.280
我々はそれを増やす場合は、モデルがより保
守的になる。

61
00:04:26.280 --> 00:04:29.180
これを0に設定すると、最小値は

62
00:04:29.180 --> 00:04:32.220
このパラメータを指定すると、モデルの制約
は小さくなります。

63
00:04:33.560 --> 00:04:34.360
私の経験では、

64
00:04:34.360 --> 00:04:39.910
それは XGBoost と
LightGBM でチューニングするため
の最も重要なパラメータの一つです。

65
00:04:39.910 --> 00:04:45.041
タスクに応じて、私は最適な値を見つける
0, 5,

66
00:04:45.041 --> 00:04:52.653
15、300ので、値の広い範囲を試すこと
を躊躇しないでください、それはデータに依
存します。

67
00:04:54.855 --> 00:04:58.384
このために使用されているハイパーパラメー
タを議論していた

68
00:04:58.384 --> 00:04:59.190
ツリーを構築します。

69
00:05:00.400 --> 00:05:04.878
そして、次に、密に接続されている2つの非
常に重要なパラメータがあり、

70
00:05:04.878 --> 00:05:08.600
eta と num_rounds

71
00:05:08.600 --> 00:05:12.922
Eta は基本的に勾配降下のような学習の
重みです。

72
00:05:12.922 --> 00:05:18.250
num_round は、我々が実行したい
か、またはどのように多くの学習手順です

73
00:05:18.250 --> 00:05:21.506
言い換えれば、どのように多くのツリーを構
築したい。

74
00:05:21.506 --> 00:05:25.570
各イテレーションで新しいツリーが構築され
、

75
00:05:25.570 --> 00:05:29.350
学習率 eta
のモデルに追加されました。

76
00:05:29.350 --> 00:05:31.860
だから一般的には、学習率が高ければ高いほ
ど、

77
00:05:31.860 --> 00:05:37.230
より速いモデルは、列車セットに適合し、お
そらくそれは以上のフィッティングにつなが
ることができます。

78
00:05:37.230 --> 00:05:41.490
そして、より多くのステップのモデルは、よ
り良いモデルが収まる。

79
00:05:42.550 --> 00:05:44.540
しかし、ここにいくつかの注意事項がありま
す。

80
00:05:45.670 --> 00:05:51.100
それはあまりにも高い学習率では、モデルが
まったく適合しないことが起こる

81
00:05:51.100 --> 00:05:52.620
収束しないだけでしょう。

82
00:05:53.730 --> 00:05:58.420
だからまず、我々は十分に小さい学習率を使
用しているかどうかを確認する必要がありま
す。

83
00:05:59.450 --> 00:06:02.420
一方、学習率が小さすぎると、

84
00:06:02.420 --> 00:06:07.300
モデルは、ラウンドの多くの後に何を学ぶこ
とができます。

85
00:06:08.600 --> 00:06:13.420
しかし、同時に、小さな学習率は、しばしば
より良い汎化につながる。

86
00:06:14.600 --> 00:06:19.280
だから、学習率がちょうどいいはずだという
ことを意味するので、

87
00:06:19.280 --> 00:06:22.400
モデルが一般化し、列車に永遠にかからない
こと。

88
00:06:24.431 --> 00:06:30.966
良いことは、我々は合理的に小さい、と言う
、0.1 または eta
のを凍結することができます

89
00:06:30.966 --> 00:06:37.917
0.01、そしてどのように多くのラウンド
は、モデルを訓練する必要があります見つけ
るゴマそれ以上のフィット。

90
00:06:37.917 --> 00:06:40.960
我々は通常、それのために早期停止を使用し
ます。

91
00:06:40.960 --> 00:06:45.860
我々は、検証の損失を監視し、損失が上がる
ことを開始したときにトレーニングを終了し
ます。

92
00:06:47.170 --> 00:06:51.690
適切な数のラウンドを見つけた時

93
00:06:51.690 --> 00:06:55.180
我々は、通常、スコアを向上させるトリック
を行うことができます。

94
00:06:55.180 --> 00:06:59.990
我々は、アルファの要因によってステップの
数を乗算し、

95
00:06:59.990 --> 00:07:04.180
同時に、我々はアルファの要因によって
eta を分割します。

96
00:07:05.280 --> 00:07:10.310
たとえば、手順の数を2倍にして、eta
を2. で除算します。

97
00:07:10.310 --> 00:07:13.720
この場合、学習には時間が2倍に長くかかり
ますが、

98
00:07:13.720 --> 00:07:16.080
結果のモデルは通常よりよくなります。

99
00:07:17.230 --> 00:07:21.190
これは、有効なパラメータも調整する必要が
ありますが発生する可能性があります

100
00:07:21.190 --> 00:07:23.600
しかし、通常はそのままにしても大丈夫です
。

101
00:07:24.610 --> 00:07:28.850
最後に、ランダムシード引数を使用すること
もできますが、

102
00:07:28.850 --> 00:07:31.590
多くの人が手の前に種子を修正することをお
勧め.

103
00:07:33.162 --> 00:07:36.918
私はそれが XGBoost でシードを修
正するためにあまり意味をなさないと思う、

104
00:07:36.918 --> 00:07:42.520
とにかくすべての変更されたパラメータは完
全に異なるモデルにつながるとして。

105
00:07:42.520 --> 00:07:46.210
しかし、私はこのパラメータを使用すること
を確認する

106
00:07:46.210 --> 00:07:49.807
異なるランダムな種子は、多くのトレーニン
グ結果を変更しないでください。

107
00:07:49.807 --> 00:07:55.850
と言う [聞こえない] 競争は、1つは1
000の場所をジャンプすることができます
か

108
00:07:55.850 --> 00:08:02.660
別のランダムな種子を持つモデルを訓練する
だけで、リーダーボードにダウン。

109
00:08:02.660 --> 00:08:06.310
ランダムシードがあまりにも多くのモデルに
影響しない場合は、良い。

110
00:08:07.510 --> 00:08:11.550
他のケースでは、私はそれが良いアイデアな
ら、1つのより多くの時間を考えることをお
勧め

111
00:08:11.550 --> 00:08:16.420
結果は非常にランダムにすることができます
ように、その競争に参加する。

112
00:08:17.530 --> 00:08:21.120
または少なくとも私はあなたのための検証ス
キームとアカウントを調整することをお勧め

113
00:08:21.120 --> 00:08:21.750
乱数。

114
00:08:22.860 --> 00:08:26.370
グラデーションのブーストが終わったわ

115
00:08:26.370 --> 00:08:29.940
さあ RandomForest と
ExtraTrees に行こう

116
00:08:31.430 --> 00:08:36.000
実際は、ExtraTrees は
RandomForest
のちょうどより無作為にされた版であり、

117
00:08:36.000 --> 00:08:37.400
は同じパラメータを持ちます。

118
00:08:37.400 --> 00:08:42.120
だから私は RandomForest
の両方のモデルの意味を言うだろう。

119
00:08:42.120 --> 00:08:46.230
RandomForest と
ExtraBoost
は木を造り、木を次々と造る。

120
00:08:47.310 --> 00:08:52.580
しかし、RandomForest は、他
の独立している各ツリーを構築します。

121
00:08:52.580 --> 00:08:56.850
それは多くの木を持っていることがのための
過食につながらないことを意味する

122
00:08:56.850 --> 00:08:59.738
勾配ブーストとは対照的に
RandomForest。

123
00:09:01.480 --> 00:09:03.420
sklearn では、木の数

124
00:09:03.420 --> 00:09:09.091
ランダムフォレストは
N_estimators
パラメータによって制御されます。

125
00:09:09.091 --> 00:09:10.500
開始時に、

126
00:09:10.500 --> 00:09:15.820
我々は、木の数は、持っているのに十分であ
るかを判断することがあります。

127
00:09:15.820 --> 00:09:21.230
ということは、それ以上のものを使うと、結
果はあまり変わらないでしょうから、

128
00:09:21.230 --> 00:09:23.890
しかし、モデルが長く収まるようになります
。

129
00:09:25.020 --> 00:09:29.314
私は通常、最初の非常に小さな数に
N_estimators
を設定すると、10と言う

130
00:09:29.314 --> 00:09:33.020
また、そのデータに10個のツリーを収める
のにどのくらい時間がかかるかを確認します
。

131
00:09:34.620 --> 00:09:39.720
それはあまりにも長いし、私は巨大な値に
N_estimators
を設定されていない場合は、

132
00:09:39.720 --> 00:09:42.950
300と言うが、それは実際に依存します。

133
00:09:44.000 --> 00:09:45.020
そして、モデルを養う。

134
00:09:46.160 --> 00:09:51.038
そして、私はどのように検証エラーの数に応
じて変更をプロット

135
00:09:51.038 --> 00:09:52.840
使用される木。

136
00:09:52.840 --> 00:09:54.470
このプロットは、通常、そのようになります
。

137
00:09:55.480 --> 00:10:02.040
我々は、x 軸上の木の数と y
軸の精度スコアを持っています。

138
00:10:02.040 --> 00:10:06.740
我々はここでは約50の木はすでに合理的な
スコアを与える参照してください

139
00:10:06.740 --> 00:10:11.830
パラメータをチューニングするときに、より
多くのものを使用する必要はありません。

140
00:10:11.830 --> 00:10:15.486
それは50の木を使用することはかなり信頼
できる。

141
00:10:15.486 --> 00:10:17.880
リーダーボードに提出する前に、

142
00:10:17.880 --> 00:10:21.670
念のために N_estimators
を高い値に設定することができます。

143
00:10:23.410 --> 00:10:26.338
このプロットのためのコードを、実際には、
読書材料で見つけることができます。

144
00:10:26.338 --> 00:10:28.079
XGBoost と同様に、

145
00:10:28.079 --> 00:10:33.500
ツリーの深度を制御するパラメータ
max_depth があります。

146
00:10:33.500 --> 00:10:35.830
しかし、別様に XGBoost に、

147
00:10:35.830 --> 00:10:40.040
これは、無制限の深さに対応する none
に設定することができます。

148
00:10:41.370 --> 00:10:45.720
これは、データセット内の機能が繰り返され
ている場合、実際には非常に便利なことがで
きます

149
00:10:45.720 --> 00:10:49.330
値と重要な相互作用。

150
00:10:49.330 --> 00:10:49.960
他のケースでは、

151
00:10:49.960 --> 00:10:54.445
制限のない深さのモデルは、すぐに収まりま
す。

152
00:10:54.445 --> 00:10:59.870
私はあなたがランダムな森林の深さ7程度か
ら始めることをお勧めします。

153
00:11:01.290 --> 00:11:05.588
通常、ランダムフォレストの最適な深度は、

154
00:11:05.588 --> 00:11:11.133
グラデーションブースト, ので、深さを試
すことを躊躇しないでください 10,
20, と高い.

155
00:11:11.133 --> 00:11:17.929
Max_features
は、XGBoost からの呼び出しサンプ
ルパラメーターに似ています。

156
00:11:17.929 --> 00:11:22.200
私は分割を解読するために使用するより多く
の機能は、より速く訓練。

157
00:11:22.200 --> 00:11:29.159
しかし、一方で、あなたはあまりにも少ない
機能を使用する必要はありません。

158
00:11:29.159 --> 00:11:33.967
min_samples_leaf
と同様の正則化パラメータです。

159
00:11:33.967 --> 00:11:39.990
XGBoost から
min_child_weight と
min_data_leaf と同じ
LightGPM から。

160
00:11:41.140 --> 00:11:45.702
ランダムな森林分類器のために、我々は
eleviate
に基準を選択することができます

161
00:11:45.702 --> 00:11:48.860
基準パラメータを持つツリー内の分割。

162
00:11:50.140 --> 00:11:52.810
それはジニまたはエントロピーのどちらかで
ある場合もある。

163
00:11:54.040 --> 00:11:57.450
1つを選択するには、我々だけの両方を試し
てみて、最高の1つを選ぶ必要があります。

164
00:11:58.520 --> 00:12:03.569
私の経験でジニはよりよくより頻繁にあるが
、時々エントロピーは勝つ。

165
00:12:05.490 --> 00:12:09.990
必要に応じて、random_state 
パラメータを使用してランダムシードを修正
することもできます。

166
00:12:11.180 --> 00:12:16.940
そして最後に、あなたが持っているコアの数
に n_jobs パラメータを設定するこ
とを忘れないでください。

167
00:12:16.940 --> 00:12:22.280
デフォルトでは、sklearn からの
RandomForest は何らかの理由
で1つのコアのみを使用します。

168
00:12:23.560 --> 00:12:28.300
だから、このビデオでは、我々は様々なハイ
パーパラメータについて話していた

169
00:12:28.300 --> 00:12:31.420
グラデーションブーストとデシジョンツリー
、およびランダムフォレストの。

170
00:12:32.590 --> 00:12:38.033
次のビデオでは、

171
00:12:38.033 --> 00:12:43.001
ニューラルネットワークと線形モデル

172
00:12:43.001 --> 00:12:49.789
音楽

