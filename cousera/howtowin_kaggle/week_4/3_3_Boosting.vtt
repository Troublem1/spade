WEBVTT

1
00:00:00.000 --> 00:00:03.835
Hello, everyone. This is Marios Michailidis.

2
00:00:03.835 --> 00:00:08.775
And today, we'll continue our discussion with ensemble methods,

3
00:00:08.775 --> 00:00:14.340
and specifically, with a very popular form of ensembling called boosting.

4
00:00:14.340 --> 00:00:16.890
What is boosting?

5
00:00:16.890 --> 00:00:22.800
Boosting is a form of weighted averaging of models where each model is built

6
00:00:22.800 --> 00:00:30.283
sequentially in a way that it takes into account previous model performance.

7
00:00:30.283 --> 00:00:32.530
In order to understand this better,

8
00:00:32.530 --> 00:00:36.333
remember that before, we discussed about biking,

9
00:00:36.333 --> 00:00:40.680
and we saw that we can have it at many different models,

10
00:00:40.680 --> 00:00:45.420
which are independent to each other in order to get a better prediction.

11
00:00:45.420 --> 00:00:47.400
Boosting does something different.

12
00:00:47.400 --> 00:00:52.070
It says, now I tried to make a model,

13
00:00:52.070 --> 00:00:55.260
but I take into account how well

14
00:00:55.260 --> 00:01:00.460
the previous models have done in order to make a better prediction.

15
00:01:00.460 --> 00:01:04.745
So, every model we add sequentially to the ensemble,

16
00:01:04.745 --> 00:01:07.510
it takes into account how well

17
00:01:07.510 --> 00:01:12.515
the previous models have done in order to make better predictions.

18
00:01:12.515 --> 00:01:18.370
There are two main boosting type of algorithms.

19
00:01:18.370 --> 00:01:19.870
One is based on weight,

20
00:01:19.870 --> 00:01:22.630
and the other is based on residual error,

21
00:01:22.630 --> 00:01:25.641
and we will discuss both of them one by one.

22
00:01:25.641 --> 00:01:33.380
For weight boosting, it's better to see an example in order to understand it better.

23
00:01:33.380 --> 00:01:38.975
Let's say we have a tabular data set, with four features.

24
00:01:38.975 --> 00:01:40.230
Let's call them x0,

25
00:01:40.230 --> 00:01:42.260
x1, x2, and x3,

26
00:01:42.260 --> 00:01:48.200
and we want to use these features to predict a target variable, y.

27
00:01:48.200 --> 00:01:51.266
What we are going to do in weight boosting is,

28
00:01:51.266 --> 00:01:53.915
we are going to fit a model,

29
00:01:53.915 --> 00:01:56.655
and we will generate predictions.

30
00:01:56.655 --> 00:01:58.280
Let's call them pred.

31
00:01:58.280 --> 00:02:01.607
These predictions have a certain margin of error.

32
00:02:01.607 --> 00:02:05.300
We can calculate these absolute error,

33
00:02:05.300 --> 00:02:07.595
and when I say absolute error,

34
00:02:07.595 --> 00:02:13.150
is absolute of y minus our prediction.

35
00:02:13.150 --> 00:02:18.360
You can see there are predictions which are very, very far off,

36
00:02:18.360 --> 00:02:21.444
like row number five,

37
00:02:21.444 --> 00:02:23.962
but there are others like number six,

38
00:02:23.962 --> 00:02:28.015
which the model has actually done quite well.

39
00:02:28.015 --> 00:02:32.480
So what we do based on this is we generate,

40
00:02:32.480 --> 00:02:33.978
let's say, a new column or a new vector,

41
00:02:33.978 --> 00:02:39.930
where we create a weight column,

42
00:02:39.930 --> 00:02:46.905
and we say that this weight is 1 plus the absolute error.

43
00:02:46.905 --> 00:02:51.945
There are different ways to calculate this weight.

44
00:02:51.945 --> 00:02:55.555
Now, I'm just giving you this as an example.

45
00:02:55.555 --> 00:02:59.005
You can infer that there are different ways to do this,

46
00:02:59.005 --> 00:03:03.270
but the overall principle is very similar.

47
00:03:03.270 --> 00:03:06.115
So what you're going to do next is,

48
00:03:06.115 --> 00:03:12.165
you're going to fit a new model using the same features and the same target variable,

49
00:03:12.165 --> 00:03:16.330
but you're going to also add this weight.

50
00:03:16.330 --> 00:03:19.675
What weight says to the model is,

51
00:03:19.675 --> 00:03:24.225
I want you to put more significance into a certain role.

52
00:03:24.225 --> 00:03:28.240
You can almost interpret weight has the number of

53
00:03:28.240 --> 00:03:32.650
times that a certain row appears in my data.

54
00:03:32.650 --> 00:03:35.039
So let's say weight was 2,

55
00:03:35.039 --> 00:03:39.400
this means that this row appears twice,

56
00:03:39.400 --> 00:03:47.707
and therefore, has bigger contribution to the total error.

57
00:03:47.707 --> 00:03:50.930
You can keep repeating this process.

58
00:03:50.930 --> 00:03:55.420
You can, again, calculate a new error based on this error.

59
00:03:55.420 --> 00:03:57.545
You calculate new weights,

60
00:03:57.545 --> 00:04:02.230
and this is how you sequentially add models to the ensemble that

61
00:04:02.230 --> 00:04:07.625
take into account how well each model has done in certain cases,

62
00:04:07.625 --> 00:04:13.805
maximizing the focus from where the previous models have done more wrong.

63
00:04:13.805 --> 00:04:18.606
There are certain parameters associated with this type of boosting.

64
00:04:18.606 --> 00:04:19.785
One is the learning rate.

65
00:04:19.785 --> 00:04:23.629
We can also call it shrinkage or eta.

66
00:04:23.629 --> 00:04:26.360
It has different names. Now, if you recall,

67
00:04:26.360 --> 00:04:30.726
I explained boosting as a form of weighted averaging.

68
00:04:30.726 --> 00:04:31.730
And this is true,

69
00:04:31.730 --> 00:04:34.825
because normally what this learning rate.

70
00:04:34.825 --> 00:04:37.220
So what we say is,

71
00:04:37.220 --> 00:04:39.024
every new model we built,

72
00:04:39.024 --> 00:04:41.600
we shouldn't trust it 100%.

73
00:04:41.600 --> 00:04:44.285
We should trust it a little bit.

74
00:04:44.285 --> 00:04:51.205
This ensures that we don't have one model generally having too much contribution,

75
00:04:51.205 --> 00:04:57.740
and completely making something that is not very generalizable.

76
00:04:57.740 --> 00:05:01.705
So this ensures that we don't over-trust one model,

77
00:05:01.705 --> 00:05:04.250
we trust many models a little bit.

78
00:05:04.250 --> 00:05:08.520
It is very good to control over fitting.

79
00:05:08.520 --> 00:05:12.625
The second parameter we look at is the number of estimators.

80
00:05:12.625 --> 00:05:14.430
This is quite important.

81
00:05:14.430 --> 00:05:18.400
And normally, there is an inverse relationship,

82
00:05:18.400 --> 00:05:20.335
an opposite relationship, with the learning rate.

83
00:05:20.335 --> 00:05:25.530
So the more estimators we add to these type of ensemble,

84
00:05:25.530 --> 00:05:28.885
the smaller learning rate we need to put.

85
00:05:28.885 --> 00:05:32.629
This is sometimes quite difficult to find the right values,

86
00:05:32.629 --> 00:05:36.345
and we do it with the help of cross-validation.

87
00:05:36.345 --> 00:05:42.730
So normally, we start with a fixed number of estimators, let's say,

88
00:05:42.730 --> 00:05:50.495
100, and then, we try to find the optimal learning rate for this 100 estimators.

89
00:05:50.495 --> 00:05:54.175
Let's say, based on cross-validation performance,

90
00:05:54.175 --> 00:05:57.335
we find this to be 0.1.

91
00:05:57.335 --> 00:05:59.700
What we can do then is,

92
00:05:59.700 --> 00:06:03.611
let's say, we can double the number of estimators,

93
00:06:03.611 --> 00:06:09.310
make it 200, and divide the learning rate by 2,

94
00:06:09.310 --> 00:06:12.185
so we can put 0.05,

95
00:06:12.185 --> 00:06:14.360
and then we take performance.

96
00:06:14.360 --> 00:06:19.627
It may be that the relationship is not as linear as I explained,

97
00:06:19.627 --> 00:06:28.590
and the best learning rate may be 0.04 or 0.06 after duplicating the estimators,

98
00:06:28.590 --> 00:06:30.295
but this is roughly the logic.

99
00:06:30.295 --> 00:06:33.820
This is how we work in order to increase estimators,

100
00:06:33.820 --> 00:06:40.262
and try to see more estimators give us better performance without losing so much time,

101
00:06:40.262 --> 00:06:45.040
every time, trying to find the best learning rate.

102
00:06:45.040 --> 00:06:49.165
Another thing we look at is the type of input model.

103
00:06:49.165 --> 00:06:55.245
And generally, we can perform boosting with any type of estimator.

104
00:06:55.245 --> 00:07:00.505
The only condition is that it needs to accept weight in it's modeling process.

105
00:07:00.505 --> 00:07:05.130
So I weigh to say how much we should rely in each role of our data set.

106
00:07:05.130 --> 00:07:10.410
And then, we have various boosting types.

107
00:07:10.410 --> 00:07:14.840
As I said, I roughly explained to you how we can use

108
00:07:14.840 --> 00:07:20.820
the weight as a means to focus on different rows,

109
00:07:20.820 --> 00:07:23.420
different cases the model has done wrong,

110
00:07:23.420 --> 00:07:25.970
but there are different ways to express this.

111
00:07:25.970 --> 00:07:28.160
For example, there are certain boosting algorithm

112
00:07:28.160 --> 00:07:31.278
that do not care about the margin of error,

113
00:07:31.278 --> 00:07:36.075
they only care if you did the classification correct or not.

114
00:07:36.075 --> 00:07:38.595
So there are different variations.

115
00:07:38.595 --> 00:07:41.720
One I really like is the AdaBoost,

116
00:07:41.720 --> 00:07:45.230
and there is a very good implementation in sklearn,

117
00:07:45.230 --> 00:07:46.675
where you can choose any input algorithm.

118
00:07:46.675 --> 00:07:49.360
I think it's really good.

119
00:07:49.360 --> 00:07:53.330
And another one I really like is, normally,

120
00:07:53.330 --> 00:07:56.313
it's only good for logistic regression,

121
00:07:56.313 --> 00:08:02.880
and there is a very good implementation in Weka for Java if you want to try.

122
00:08:02.880 --> 00:08:07.230
Now, let's move onto the our time of boosting,

123
00:08:07.230 --> 00:08:10.130
which has been the most successful.

124
00:08:10.130 --> 00:08:15.820
I believe that in any predictive modeling competition that

125
00:08:15.820 --> 00:08:22.745
was not image classification or predicting videos.

126
00:08:22.745 --> 00:08:27.850
This has been the most dominant type of algorithm that actually has one most in

127
00:08:27.850 --> 00:08:33.922
these challenges so this type of boosting has been extremely successful,

128
00:08:33.922 --> 00:08:35.650
but what is it?

129
00:08:35.650 --> 00:08:42.580
I'll try to give you again a similar example in order to understand the concept.

130
00:08:42.580 --> 00:08:46.605
Let's say we have again the same dataset, same features,

131
00:08:46.605 --> 00:08:48.778
again when trying to predict a y variable,

132
00:08:48.778 --> 00:08:53.390
we fit a model, we make predictions.

133
00:08:53.390 --> 00:08:55.160
What we do next,

134
00:08:55.160 --> 00:08:58.925
is we'll calculate the error of these predictions but this time,

135
00:08:58.925 --> 00:09:04.725
not in absolute terms because we're interested about the direction of the error.

136
00:09:04.725 --> 00:09:13.420
What we do next is we take this error and we make it adding

137
00:09:13.420 --> 00:09:18.955
new y variable so the error now becomes

138
00:09:18.955 --> 00:09:26.290
the new target variable and we use the same features in order to predict this error.

139
00:09:26.290 --> 00:09:29.590
It's an interesting concept and if we wanted,

140
00:09:29.590 --> 00:09:34.420
let's say to make predictions for Rownum equals one,

141
00:09:34.420 --> 00:09:37.405
what we would do is we will take

142
00:09:37.405 --> 00:09:43.855
our initial prediction and then we'll add the new prediction,

143
00:09:43.855 --> 00:09:48.220
which is based on the error of the first prediction.

144
00:09:48.220 --> 00:09:54.190
So initially, we have 0.75 and then we predicted 0.2.

145
00:09:54.190 --> 00:09:57.220
In order to make a final prediction,

146
00:09:57.220 --> 00:10:02.060
we would say one plus the other equals 0.95.

147
00:10:02.060 --> 00:10:06.385
If you recall, the target for this row, it was one.

148
00:10:06.385 --> 00:10:11.720
Using two models, we were able to get closer to the actual answer.

149
00:10:11.720 --> 00:10:14.560
This form of boosting works really,

150
00:10:14.560 --> 00:10:18.630
really well to minimize the error.

151
00:10:18.630 --> 00:10:24.215
There are certain parameters again which are associated with this type of boosting.

152
00:10:24.215 --> 00:10:29.990
The first is again the learning rate and it works pretty much as I explained it before.

153
00:10:29.990 --> 00:10:36.535
What you need to take into account is how this is applied.

154
00:10:36.535 --> 00:10:41.045
Let's say we have a learning rate of 0.1.

155
00:10:41.045 --> 00:10:43.160
In the previous example,

156
00:10:43.160 --> 00:10:49.850
where the prediction was 0.2 for the second model,

157
00:10:49.850 --> 00:10:58.970
what you will say is I want to move my prediction towards that direction only 10 percent.

158
00:10:58.970 --> 00:11:01.860
If you remember the prediction was 0.2,

159
00:11:01.860 --> 00:11:05.590
10 percent of this is 0.02.

160
00:11:05.590 --> 00:11:11.160
This is how much we would move towards the prediction of the error.

161
00:11:11.160 --> 00:11:13.750
This is a good way to control over fitting.

162
00:11:13.750 --> 00:11:16.890
Again, we ensure we don't over rely in one model.

163
00:11:16.890 --> 00:11:22.935
Again, how many estimators you put is quite important.

164
00:11:22.935 --> 00:11:28.705
Normally, more is better but you need to offset this with the right learning rate.

165
00:11:28.705 --> 00:11:34.615
You need to make certain that every model has the right contribution.

166
00:11:34.615 --> 00:11:36.656
If you intent to put many,

167
00:11:36.656 --> 00:11:41.780
then you need to make sure that your models have very, very small contribution.

168
00:11:41.780 --> 00:11:44.135
Again, you decide these parameters based on

169
00:11:44.135 --> 00:11:49.355
cross-validation and the logic is very similar as explained before.

170
00:11:49.355 --> 00:11:54.115
Other things that work really well is taking

171
00:11:54.115 --> 00:11:59.445
a subset of rows or a subset of columns when you build its model.

172
00:11:59.445 --> 00:12:05.773
Actually, there is no reason why we wouldn't use this with the previous algorithm.

173
00:12:05.773 --> 00:12:06.806
The way its based,

174
00:12:06.806 --> 00:12:10.895
it is more common with this type of boosting,

175
00:12:10.895 --> 00:12:14.185
and internally works quite well.

176
00:12:14.185 --> 00:12:18.310
For input model, I have seen that this method

177
00:12:18.310 --> 00:12:22.330
works really well with this increase but theoretically,

178
00:12:22.330 --> 00:12:24.360
you can put anything you want.

179
00:12:24.360 --> 00:12:27.310
Again, there are various boosting types.

180
00:12:27.310 --> 00:12:32.570
I think the two most common or more successful right now in

181
00:12:32.570 --> 00:12:38.245
a predictive modeling context is the gradient based,

182
00:12:38.245 --> 00:12:43.429
which is actually what I explained with you how the prediction and you don't move

183
00:12:43.429 --> 00:12:49.045
100 percent with that direction if you apply the learning rate.

184
00:12:49.045 --> 00:12:50.690
The other very interesting one,

185
00:12:50.690 --> 00:12:53.305
which I've actually find it very efficient

186
00:12:53.305 --> 00:12:57.190
especially in classification problems is the dart.

187
00:12:57.190 --> 00:13:02.900
Dart, it imposes a drop

188
00:13:02.900 --> 00:13:09.075
out mechanism in order to control the contribution of the trees.

189
00:13:09.075 --> 00:13:13.035
This is a concept derived from deep learning where you say,

190
00:13:13.035 --> 00:13:17.180
"Every time I make a new prediction in my sample,

191
00:13:17.180 --> 00:13:21.275
every time I add a new estimate or I'm not relying on

192
00:13:21.275 --> 00:13:25.805
all previous estimators but only on a subset of them."

193
00:13:25.805 --> 00:13:27.510
Just to give you an example,

194
00:13:27.510 --> 00:13:31.660
let's say we have a drop out rate of 20 percent.

195
00:13:31.660 --> 00:13:35.060
So far, we have built 10 trees,

196
00:13:35.060 --> 00:13:40.010
we want to or 10 models and then we try to see,

197
00:13:40.010 --> 00:13:43.400
we try to build a new, an 11th one.

198
00:13:43.400 --> 00:13:49.550
What we'll do is we will randomly exclude two trees when

199
00:13:49.550 --> 00:13:55.820
we generate a prediction for that 11th tree or that 11th model.

200
00:13:55.820 --> 00:13:59.600
By randomly excluding some models,

201
00:13:59.600 --> 00:14:02.630
by introducing this kind of randomness,

202
00:14:02.630 --> 00:14:06.605
it works as a form of regularization.

203
00:14:06.605 --> 00:14:10.370
Therefore, it helps a lot to make

204
00:14:10.370 --> 00:14:14.900
a model that generalizes quite well enough for same data.

205
00:14:14.900 --> 00:14:19.006
This concept tends to work quite well

206
00:14:19.006 --> 00:14:25.285
because this type of boosting algorithm has been so successful.

207
00:14:25.285 --> 00:14:28.434
There have been many implementations to try

208
00:14:28.434 --> 00:14:32.020
to improve on different parts of these algorithms.

209
00:14:32.020 --> 00:14:34.510
One really successful application

210
00:14:34.510 --> 00:14:38.835
especially in the comparative predictive modeling world is the Xgboost.

211
00:14:38.835 --> 00:14:44.230
It is very scalable and it supports many loss functions.

212
00:14:44.230 --> 00:14:49.675
At the same time, is available in all major programming languages for data science.

213
00:14:49.675 --> 00:14:51.865
Another good implementation is Lightgbm.

214
00:14:51.865 --> 00:14:54.659
As the name connotes,

215
00:14:54.659 --> 00:14:56.620
it is lightning fast.

216
00:14:56.620 --> 00:15:03.515
Also, it is supported by many programming languages and supports many loss functions.

217
00:15:03.515 --> 00:15:07.361
Another interesting case is the Gradient Boosting Machine from H2O.

218
00:15:07.361 --> 00:15:09.760
What's really interesting about

219
00:15:09.760 --> 00:15:14.800
this implementation is that it can handle categorical variables out of

220
00:15:14.800 --> 00:15:18.620
the box and it also comes with a real set of

221
00:15:18.620 --> 00:15:24.480
parameters where you can control the modeling process quite thoroughly.

222
00:15:24.480 --> 00:15:29.110
Another interesting case, which is also fairly new is the Catboost.

223
00:15:29.110 --> 00:15:35.525
What's really good about this is that it comes with the strong initial set of parameters.

224
00:15:35.525 --> 00:15:38.980
Therefore, you don't need to spend so much time tuning.

225
00:15:38.980 --> 00:15:40.375
As I mentioned before,

226
00:15:40.375 --> 00:15:43.360
this can be quite a time consuming process.

227
00:15:43.360 --> 00:15:46.830
It can also handle categorical variables out of the box.

228
00:15:46.830 --> 00:15:52.616
Ultimately, I really like the Gradient Boosting Machine implementation of Scikit-learn.

229
00:15:52.616 --> 00:16:01.413
What I really like about this is that you can put any scikit-learn estimator as a base.

230
00:16:01.413 --> 00:16:03.835
This is the end of this video.

231
00:16:03.835 --> 00:16:05.080
In the next session,

232
00:16:05.080 --> 00:16:06.640
we will discuss docking,

233
00:16:06.640 --> 00:16:10.000
which is also very popular, so stay tuned.