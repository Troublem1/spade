1
00:00:03.080 --> 00:00:05.268
ねえ皆さん。

2
00:00:05.268 --> 00:00:10.095
今日, 我々は、データを統合する機能を視
覚化するためのこの新しい方法について説明
します.

3
00:00:10.095 --> 00:00:11.540
このビデオの最後に、

4
00:00:11.540 --> 00:00:14.190
あなたの製品で恒常化を使用することができ
ます。

5
00:00:14.190 --> 00:00:15.745
前のビデオでは、

6
00:00:15.745 --> 00:00:20.930
我々は、線形モデルに非常に近い略奪されて
いる形而上学ルネゲノン技術について学んだ
。

7
00:00:20.930 --> 00:00:22.980
このビデオでは、我々はタッチされます

8
00:00:22.980 --> 00:00:26.355
次元の縮小の非線形メソッドの対象。

9
00:00:26.355 --> 00:00:29.180
それは一般に言うマニホールド学習と呼ばれ
ています。

10
00:00:29.180 --> 00:00:34.225
たとえば、左側の文字 S
の形式でデータを確認します。

11
00:00:34.225 --> 00:00:36.380
右に、我々は実行している結果を見ることが
できます

12
00:00:36.380 --> 00:00:39.255
データのさまざまなマニホールド学習アルゴ
リズム。

13
00:00:39.255 --> 00:00:43.560
この新しい結果は、スライドの右下隅に配置
されます。

14
00:00:43.560 --> 00:00:46.803
この新しいアルゴリズムは、講義のメイント
ピックであり、

15
00:00:46.803 --> 00:00:50.170
それはどのようにこれは本当に作品ここで説
明されませんの指示として。

16
00:00:50.170 --> 00:00:54.090
しかし、詳細については、追加の資料を見に
来る。

17
00:00:54.090 --> 00:00:58.295
これは、ポイントを投影しようとするメソッ
ドであるというだけです。

18
00:00:58.295 --> 00:01:01.340
小さな次元空間への高次元空間

19
00:01:01.340 --> 00:01:05.075
ポイント間の距離がおおよそ保存されるよう
にします。

20
00:01:05.075 --> 00:01:09.500
MNIST データセットの恒常化の例を見
てみましょう。

21
00:01:09.500 --> 00:01:15.225
ここでは、2次元空間に投影された700次
元空間からのポイントがあります。

22
00:01:15.225 --> 00:01:19.235
このような射影は明示的にクラスタを形成す
ることがわかります。

23
00:01:19.235 --> 00:01:22.240
クールは、これらのクラスタが有意義である
ことを示し、

24
00:01:22.240 --> 00:01:25.785
目標数値にも対応します。

25
00:01:25.785 --> 00:01:29.400
また、近傍クラスタは、視覚的に類似した数
値に対応しています。

26
00:01:29.400 --> 00:01:32.730
たとえば、3つのクラスタのクラスタの横に
ある

27
00:01:32.730 --> 00:01:37.490
5つの偶然に6と8のクラスタに調整されま
す。

28
00:01:37.490 --> 00:01:41.535
データが MNIST データセットの場合
のように明示的な構造を持っている場合、

29
00:01:41.535 --> 00:01:44.460
これは、恒常化プロットに反映される可能性
があります。

30
00:01:44.460 --> 00:01:49.410
恒常化は、探索的データ分析で広く使用され
ている理由について。

31
00:01:49.410 --> 00:01:53.875
但し、恒常化が魔法であることを常に助ける
ことを仮定してはいけない。

32
00:01:53.875 --> 00:01:58.640
たとえば、ハイパーパラメータの不幸の選択
は、貧しい結果につながる可能性があります
。

33
00:01:58.640 --> 00:02:02.095
例を考えて、センターでは、少なくとも提示
される

34
00:02:02.095 --> 00:02:06.590
前の例とまったく同じ MNIST
データの恒常化射影

35
00:02:06.590 --> 00:02:09.340
困惑パラメータのみが変更されました。

36
00:02:09.340 --> 00:02:11.110
左側に、比較のために、

37
00:02:11.110 --> 00:02:13.225
我々は前の権利からプロットしている。

38
00:02:13.225 --> 00:02:17.190
右側にあるので、ランダムなデータの恒常化
投影を提示します。

39
00:02:17.190 --> 00:02:20.790
私達はの選択として見ることができる雑種の
変数の変化の投射

40
00:02:20.790 --> 00:02:24.500
クラスタが表示されないようにデータを大幅
に MNIST します。

41
00:02:24.500 --> 00:02:30.775
さらに、新しい投影法は、元のデータではな
く、ランダムな情報に似ています。

42
00:02:30.775 --> 00:02:34.615
困惑ハイパーパラメータ値に依存するものを
調べてみましょう。

43
00:02:34.615 --> 00:02:36.426
左側には困惑 = 3 があり、

44
00:02:36.426 --> 00:02:42.805
中央 = 10、右側 = 150。

45
00:02:42.805 --> 00:02:47.910
私は、これらの予測は、すべて同じデータの
ために作られて強調したい。

46
00:02:47.910 --> 00:02:52.875
この図は、これらの新しい結果がそのパラメ
ータに強く依存していることを示しています
。

47
00:02:52.875 --> 00:02:57.270
そして、結果の解釈は、単純なタスクではあ
りません。

48
00:02:57.270 --> 00:02:59.500
特に、1つのサイズを推測することはできま
せん

49
00:02:59.500 --> 00:03:02.855
投影されたクラスタのサイズを使用する元の
クラスタ。

50
00:03:02.855 --> 00:03:06.050
同様の命題は、クラスタ間の距離に対して有
効です。

51
00:03:06.050 --> 00:03:09.417
ブログの蒸留。パブには、ポストが含まれて

52
00:03:09.417 --> 00:03:13.595
恒常化の結果を理解し、解釈する方法につい
て。

53
00:03:13.595 --> 00:03:16.220
また、それは偉大な対話型のデモが含まれて
います

54
00:03:16.220 --> 00:03:19.575
それはあなたがどのように恒常化の作品の問
題に入るのに役立ちます。

55
00:03:19.575 --> 00:03:21.980
私はそれを見てみることを強くお勧めします
。

56
00:03:21.980 --> 00:03:24.690
探索的データ分析に加えて、

57
00:03:24.690 --> 00:03:28.770
恒常化は、データから新しい機能を取得する
方法と見なすことができます。

58
00:03:28.770 --> 00:03:33.235
トランスフォーマー座標を元のフィーチャマ
トリックスに連結するだけです。

59
00:03:33.235 --> 00:03:35.680
これを実際の詳細について聞いたら

60
00:03:35.680 --> 00:03:37.270
以前に示したように、

61
00:03:37.270 --> 00:03:38.490
恒常化アルゴリズムの結果は、

62
00:03:38.490 --> 00:03:41.480
それは強くハイパーパラメータに依存します
。

63
00:03:41.480 --> 00:03:45.690
さまざまな難事で複数の突起を使用すること
をお勧めします。

64
00:03:45.690 --> 00:03:49.110
さらに、この方法の確率のために結果

65
00:03:49.110 --> 00:03:52.660
同じデータとハイパーパラメータでも異なる
投影。

66
00:03:52.660 --> 00:03:58.490
これは、列車とテストセットを別々ではなく
一緒に投影する必要があることを意味します
。

67
00:03:58.490 --> 00:04:02.575
また、恒常化は、多くの機能を持っている場
合、長い間実行されます。

68
00:04:02.575 --> 00:04:05.290
フィーチャの数が500を超える場合は、

69
00:04:05.290 --> 00:04:09.165
あなたは、次元削減アプローチのいずれかを
使用して、機能の数を減らす必要があります

70
00:04:09.165 --> 00:04:11.585
たとえば、100にします。

71
00:04:11.585 --> 00:04:15.700
恒常化の実装は、sklearn
ライブラリにあります。

72
00:04:15.700 --> 00:04:17.255
しかし、個人的に、私は使用することを好む

73
00:04:17.255 --> 00:04:20.975
別の実装恒常化と呼ばれる別の
Python パッケージから、

74
00:04:20.975 --> 00:04:24.830
それは方法より効率的な実装を提供するため
。

75
00:04:24.830 --> 00:04:28.570
結論として、講義の基本的なポイントを思い
出させたいと思います。

76
00:04:28.570 --> 00:04:31.630
恒常化は、データを視覚化するための優れた
ツールです。

77
00:04:31.630 --> 00:04:33.785
データに明示的な構造がある場合は、

78
00:04:33.785 --> 00:04:37.318
それはおそらく恒常化投影で
[聞こえない] になります。

79
00:04:37.318 --> 00:04:41.615
しかし、それは恒常化の結果の解釈と慎重に
する必要があります。

80
00:04:41.615 --> 00:04:46.145
場合によっては、存在しないか、またはその
逆の構造を見ることができます

81
00:04:46.145 --> 00:04:48.785
構造が実際に存在する場合は none
を参照してください。

82
00:04:48.785 --> 00:04:53.530
それは別の難事でいくつかの恒常化の突起を
行うには良い練習だ。

83
00:04:53.530 --> 00:04:55.035
そして、英和に加えて、

84
00:04:55.035 --> 00:04:59.125
恒常化は、モデルを供給するための機能とし
て非常によく働いています。

85
00:04:59.125 --> 00:05:01.800
ご注意をありがとうございました。

