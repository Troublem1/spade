1
00:00:00.025 --> 00:00:03.770
音楽

2
00:00:03.770 --> 00:00:05.280
こんにちは。

3
00:00:05.280 --> 00:00:09.410
多くの場合、計算では、テキストや画像など
のデータがあります。

4
00:00:09.410 --> 00:00:14.110
これらのデータのみをお持ちの場合は、この
タイプの情報に固有のアプローチを適用する
ことができます。

5
00:00:14.110 --> 00:00:19.350
例えば、我々は同様のテキストを見つけるた
めに検索エンジンを使用することができます
。

6
00:00:19.350 --> 00:00:23.140
それは例えばアレン AI
の挑戦の場合だった。

7
00:00:23.140 --> 00:00:26.928
画像については、一方で、我々は、条件付き
ニューラルネットワークを使用することがで
きます

8
00:00:26.928 --> 00:00:31.550
データサイエンスボウルのように、他の大会
の全体の束。

9
00:00:31.550 --> 00:00:33.350
しかし、もし我々のテキストや

10
00:00:33.350 --> 00:00:38.340
画像を追加データとして、我々は通常、さま
ざまな機能を把握する必要があります

11
00:00:38.340 --> 00:00:44.030
サンプルおよび特徴の私達の主要なデータフ
レームに補足として編集することができる。

12
00:00:44.030 --> 00:00:48.353
我々が呼んでいるタイタニックデータセット
で見ることができるようなケースの非常に簡
単な例

13
00:00:48.353 --> 00:00:50.710
名前は、テキストのようになります。

14
00:00:50.710 --> 00:00:54.195
それを使用するには、まず、そこから便利な
機能を派生する必要があります。

15
00:00:54.195 --> 00:00:58.731
もう一つの最も確実の例では、我々はかどう
かを予測することができますオンラインのペ
ア

16
00:00:58.731 --> 00:01:04.050
広告は、お互いの異なるコピーを
slighty のように、重複している

17
00:01:04.050 --> 00:01:09.056
そして、我々は無料のデータとして、これら
の広告から画像を持つことが

18
00:01:09.056 --> 00:01:12.754
Avito
重複広告の検出の競争のように。

19
00:01:12.754 --> 00:01:15.560
または、ドキュメントを分類するタスクを与
えられるかもしれませんが、

20
00:01:15.560 --> 00:01:18.490
Tradeshift
のテキスト分類の挑戦のように。

21
00:01:19.870 --> 00:01:24.290
フィーチャの抽出が完了すると、抽出された
フィーチャを異なる方法で扱うことができま
す。

22
00:01:24.290 --> 00:01:28.510
場合によっては、既存のデータフレームに新
しい機能を追加したいだけです。

23
00:01:28.510 --> 00:01:32.753
場合によっては、適切な機能を個別に使用す
ることもできますが、最終的には、

24
00:01:32.753 --> 00:01:34.702
ベースソリューションでの持ち分を確認しま
す。

25
00:01:34.702 --> 00:01:39.660
我々は、ステークを通過し、我々はどのよう
にトピックの後でそれを適用する方法を学び
ます

26
00:01:39.660 --> 00:01:45.013
アンサンブルについては、今のところ、あな
たは、両方の方法を最初に取得することを知
っている必要があります

27
00:01:45.013 --> 00:01:49.050
もちろん、テキストや画像から何らかの機能
を抽出します。

28
00:01:49.050 --> 00:01:52.280
そして、これはまさに我々はこのビデオで議
論するものです。

29
00:01:53.510 --> 00:01:56.760
テキストからの特色抽出から始めましょう。

30
00:01:56.760 --> 00:01:58.646
これを行うには、主に2つの方法があります
。

31
00:01:58.646 --> 00:02:02.002
まず、言葉の袋を適用することです,
と2番目の,

32
00:02:02.002 --> 00:02:05.720
ベクターには、込みのように使用します。

33
00:02:05.720 --> 00:02:09.840
ここでは、これらの各方法について少し説明
し、

34
00:02:09.840 --> 00:02:14.140
また、それらに関連するテキスト前処理を行
っていきます。

35
00:02:15.500 --> 00:02:19.350
最初のアプローチ、最も単純なもの、言葉の
バッグから始めましょう。

36
00:02:20.410 --> 00:02:23.010
ここでは、新しい列を作成します。

37
00:02:23.010 --> 00:02:28.480
データからの各ユニークな単語は、我々は単
に出現の数を数える

38
00:02:28.480 --> 00:02:32.460
それぞれの単語を指定し、この値を適切な列
に配置します。

39
00:02:32.460 --> 00:02:34.310
各行に分離を適用した後、

40
00:02:34.310 --> 00:02:39.210
我々は、サンプルと機能の通常のデータフレ
ームを持つことになります。

41
00:02:39.210 --> 00:02:42.305
スカラーでは、これは
CountVectorizer
で行うことができます。

42
00:02:42.305 --> 00:02:48.940
また、事前に定義されたメソッドを使用して
、プロセスの計算済みメトリックを転記でき
ます。

43
00:02:48.940 --> 00:02:55.400
我々は後処理を必要とする理由を確認するに
は kNN のようないくつかのモデルを覚
えてみましょう、

44
00:02:55.400 --> 00:03:00.730
神経回帰と同様に、ニューラルネットワーク
は、機能のスケーリングに依存します。

45
00:03:00.730 --> 00:03:03.280
だから後処理の主な目標はここ

46
00:03:03.280 --> 00:03:07.100
サンプルを1つの側面でより対等にさせるこ
とであり、他では、

47
00:03:07.100 --> 00:03:11.360
無駄なもののスケールを減少させながら、よ
り重要機能をブースト。

48
00:03:12.480 --> 00:03:16.310
1つの方法は、サンプルを作るの最初の目標
を達成するために小さな匹敵する

49
00:03:16.310 --> 00:03:19.530
は、行の値の合計を正規化することです。

50
00:03:19.530 --> 00:03:25.130
このようにして、我々は出現ではなく、単語
の頻度を数えます。

51
00:03:25.130 --> 00:03:29.330
従って、異なったサイズのテキストはより対
等である。

52
00:03:29.330 --> 00:03:32.640
これは、用語周波数変換の正確な目的です。

53
00:03:34.012 --> 00:03:38.080
2番目の目標を達成するためには、より重要
な機能を高めるためには、

54
00:03:38.080 --> 00:03:43.590
データ列を正規化することで、ポストプロセ
スをマトリックスにします。

55
00:03:43.590 --> 00:03:48.810
良いアイデアは、ドキュメントの逆分数によ
って、各機能を正規化することです

56
00:03:48.810 --> 00:03:51.740
この機能に対応する正確な単語が含まれてい
ます。

57
00:03:52.840 --> 00:03:56.210
この場合、頻出語に対応する機能

58
00:03:56.210 --> 00:03:59.960
は、より稀な単語に対応する機能と比較して
縮小されます。

59
00:04:01.090 --> 00:04:05.289
我々は、これらの対数を取ることにより、こ
のアイデアをさらに改善することができます

60
00:04:05.289 --> 00:04:07.220
numberization 係数。

61
00:04:07.220 --> 00:04:11.410
結果として、これは広範な単語の重要性を減
少させる

62
00:04:11.410 --> 00:04:15.110
データセットと do
には、機能のスケーリングが必要です。

63
00:04:15.110 --> 00:04:18.540
これは、逆ドキュメントの周波数変換の目的
です。

64
00:04:19.660 --> 00:04:24.631
一般的な周波数、および逆ドキュメントの周
波数変換、

65
00:04:24.631 --> 00:04:30.050
多くの場合、sklearn
のように、TFiDF
ベクトライザーで一緒に使用されます。

66
00:04:30.050 --> 00:04:34.070
前の例に TFiDF
変換を適用してみましょう。

67
00:04:34.070 --> 00:04:36.180
まず、TF。

68
00:04:36.180 --> 00:04:36.833
素晴らしい。

69
00:04:36.833 --> 00:04:39.652
周波数に切り替わる出現

70
00:04:39.652 --> 00:04:43.108
つまり、各行の分散の一部が1に等しくなり
ます。

71
00:04:43.108 --> 00:04:44.019
さて、IDF は、素晴らしい。

72
00:04:44.019 --> 00:04:48.640
今のデータは、正規化された列の賢明な、あ
なたが見ることができる、

73
00:04:48.640 --> 00:04:51.335
あまりにも興奮している方、

74
00:04:51.335 --> 00:04:56.160
IDF
変換は、適切な機能を縮小しました。

75
00:04:57.470 --> 00:05:01.970
それは多くの TFiDF
の他の亜種があることを言及する価値がある

76
00:05:01.970 --> 00:05:05.429
これは、特定のデータに応じてより良い仕事
があります。

77
00:05:05.429 --> 00:05:08.860
もう一つの非常に有用なテクニックは
Ngrams です。

78
00:05:08.860 --> 00:05:13.710
Ngram の概念は簡単です, あなたは
、単語に対応する列だけでなく、追加,

79
00:05:13.710 --> 00:05:17.340
inconsequent
語に対応する列もあります。

80
00:05:17.340 --> 00:05:21.092
この概念は、文字のシーケンスにも適用でき

81
00:05:21.092 --> 00:05:26.465
n が低い場合は、n 文字の可能な組み合
わせごとに列があります。

82
00:05:26.465 --> 00:05:30.840
我々が見ることができるように、N = 1
の場合、これらの列の数は28に等しくなり
ます。

83
00:05:30.840 --> 00:05:36.220
N = 2 の場合、これらの列の数を計算
してみましょう。

84
00:05:36.220 --> 00:05:38.890
まあ、それは28乗されます。

85
00:05:38.890 --> 00:05:43.681
時にはそれがすべての可能な char
Ngram を持って安くすることができま
す注意してください

86
00:05:43.681 --> 00:05:48.876
データセットから一意の単語ごとに機能を持
つのではなく、機能。

87
00:05:48.876 --> 00:05:54.260
char Ngrams を使用しても、目
に見えない言葉を処理する私たちのモデルに
役立ちます。

88
00:05:54.260 --> 00:05:57.389
たとえば、既に使用されている単語のまれな
形式。

89
00:05:58.640 --> 00:06:02.999
scalared カウント
vectorizor
に適切なパラメータがあります。

90
00:06:02.999 --> 00:06:06.140
Ngrams
を使用すると、Ngram_range
と呼ばれます。

91
00:06:06.140 --> 00:06:12.598
word Ngrams から char
Ngrams
に変更するには、analyzer
という名前のパラメーターを使用します。

92
00:06:12.598 --> 00:06:18.020
通常、テキストを前処理する場合でも、単語
のバッグを適用する前に、

93
00:06:18.020 --> 00:06:22.860
そして時には、慎重なテキストの前処理は、
単語の袋を大幅に助けることができる。

94
00:06:23.940 --> 00:06:28.030
ここでは、テキストを小文字に変換する方法
について説明します。

95
00:06:28.030 --> 00:06:31.320
分類、ステミング、およびストップワードの
使用。

96
00:06:32.330 --> 00:06:36.125
小文字の有用性を示す簡単な例を考えてみま
しょう。

97
00:06:36.125 --> 00:06:41.500
私たちは非常に、非常に日当たりの良い文に
単語の袋を適用した場合はどうですか?

98
00:06:41.500 --> 00:06:44.440
我々は、各単語の3つの列を取得します。

99
00:06:44.440 --> 00:06:50.020
だから非常に、大文字で、非常にそれがなけ
れば、同じ文字列ではない、

100
00:06:50.020 --> 00:06:53.300
我々は、同じ単語の複数の列を取得し、

101
00:06:53.300 --> 00:06:57.710
繰り返しになりますが、日当たりのいい大文
字とそれなしで日当たりが一致しません。

102
00:06:57.710 --> 00:07:03.106
したがって、まず、私たちがしたいことは、
私たちのテキストに小文字を適用することで
す。

103
00:07:03.106 --> 00:07:08.740
幸いなことに、sklearn から
configurizer
は、デフォルトでこれを行います。

104
00:07:09.960 --> 00:07:12.950
さあ、分類とステミングに移りましょう。

105
00:07:14.010 --> 00:07:17.870
これらのメソッドは、より高度な前処理を参
照します。

106
00:07:17.870 --> 00:07:19.540
この例を見てみましょう。

107
00:07:19.540 --> 00:07:23.500
私は2つの文を持っている: 私は車を持っ
ていた、我々は車を持っている。

108
00:07:24.560 --> 00:07:30.580
私たちは言葉を統一することがあります車と
車は、基本的には同じ言葉です。

109
00:07:30.580 --> 00:07:33.190
同じことがあったと持っている、などのため
に行く。

110
00:07:34.270 --> 00:07:38.525
ステミングと分類の両方がこの目的を達成す
るために使用されることがありますが、

111
00:07:38.525 --> 00:07:40.730
彼らはさまざまな方法でこれを達成する。

112
00:07:41.930 --> 00:07:46.867
ステミングは、通常、ヒューリスティックな
プロセスを意味し、単語の終了をチョップと

113
00:07:46.867 --> 00:07:51.296
従って民主主義のような関連した単語の持続
期間を、民主的結合し、

114
00:07:51.296 --> 00:07:57.250
これらの単語のそれぞれのための、demo
cr のようなものを作り出す民主化。

115
00:07:57.250 --> 00:08:02.200
分類は、手には、通常、これを慎重に行うに
はしたいことを意味

116
00:08:02.200 --> 00:08:06.200
知識や語彙、および力の形態的類似性を使用
して、

117
00:08:06.200 --> 00:08:08.910
以下の言葉のそれぞれの民主主義を返す。

118
00:08:10.080 --> 00:08:14.070
ステミングとの違いを示す別の例を見てみま
しょう

119
00:08:14.070 --> 00:08:16.844
分類にそれらを適用することによって。

120
00:08:16.844 --> 00:08:21.950
ステミングは、文字 s
に戻りますが、分類は

121
00:08:21.950 --> 00:08:27.080
参照または見て、単語の意味に依存するいず
れかを返すようにしてください。

122
00:08:27.080 --> 00:08:30.340
テキストの前処理の最後の手法は、

123
00:08:30.340 --> 00:08:34.370
ここでは、ストップワードの使用法について
説明します。

124
00:08:34.370 --> 00:08:39.430
基本的に、ストップワードは、重要な情報が
含まれていない単語です。

125
00:08:39.430 --> 00:08:40.630
私たちのモデル。

126
00:08:40.630 --> 00:08:45.480
彼らはどちらかの記事や前置詞のように些細
な、または

127
00:08:45.480 --> 00:08:48.860
だから、彼らは私たちの仕事を解決するのに
役立ちません共通。

128
00:08:48.860 --> 00:08:53.280
ほとんどの言語には、定義済みのストップワ
ードのリストがあります。

129
00:08:53.280 --> 00:08:56.040
インターネット上または NLTK
からログインして、

130
00:08:56.040 --> 00:09:00.378
これは、Python のための自然言語ツ
ールキットライブラリの略です。

131
00:09:00.378 --> 00:09:06.229
sklearn からの
CountVectorizer には、ス
トップワードに関連するパラメータもあり、

132
00:09:06.229 --> 00:09:08.321
これは max_df と呼ばれています。

133
00:09:08.321 --> 00:09:11.796
max_df は、我々が見ることができる
単語のしきい値です。

134
00:09:11.796 --> 00:09:16.837
私たちが見た後、単語はテキストコーパスか
ら削除されます。

135
00:09:16.837 --> 00:09:23.680
良い, 我々は、テキストのための古典的な
特徴抽出パイプラインを議論している.

136
00:09:23.680 --> 00:09:27.420
冒頭で、我々のテキストを事前に処理する必
要があります。

137
00:09:27.420 --> 00:09:31.990
これを行うには、小文字、ステミング、分類
、または

138
00:09:31.990 --> 00:09:33.880
ストップワードを削除します。

139
00:09:33.880 --> 00:09:38.980
前処理後、我々は行列を得るために単語のア
プローチのバッグを使用することができます

140
00:09:38.980 --> 00:09:44.060
各行はテキストを表し、各列は一意の単語を
表します。

141
00:09:45.130 --> 00:09:49.393
また、我々は、Ngrams のための単語
のアプローチのバッグを使用することができ
ます

142
00:09:49.393 --> 00:09:54.504
複数の連続する単語または文字のグループの
新しい列。

143
00:09:54.504 --> 00:09:59.381
最後に、TFiDF を使用してこれらのメ
トリックを後処理すると、

144
00:09:59.381 --> 00:10:01.830
これはしばしば有用であることを証明する。

145
00:10:03.190 --> 00:10:08.310
さて、今我々の基本的なデータフレームに抽
出された機能を追加することができます

146
00:10:08.310 --> 00:10:12.350
またはいくつかのトリッキーな機能を作成す
るには、依存モデルを置く。

147
00:10:13.360 --> 00:10:14.730
それは今のところすべてです。

148
00:10:14.730 --> 00:10:19.200
次のビデオでは、引き続き機能の抽出につい
て説明します。

149
00:10:19.200 --> 00:10:21.690
我々は2つの大きなポイントを通過します。

150
00:10:21.690 --> 00:10:24.704
まず、アプローチについてお話します

151
00:10:24.704 --> 00:10:29.160
次に、画像の特徴抽出について説明します。

152
00:10:29.160 --> 00:10:35.070
音楽

