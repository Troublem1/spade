WEBVTT

1
00:00:03.150 --> 00:00:06.265
Hi, everyone. In this video,

2
00:00:06.265 --> 00:00:11.460
I want to do a brief overview of basic machine learning approaches and ideas behind them.

3
00:00:11.460 --> 00:00:15.480
There are several famous of machine learning algorithms which I want to review.

4
00:00:15.480 --> 00:00:16.765
It's a Linear Model,

5
00:00:16.765 --> 00:00:20.220
Tree-Based Methods, k-Nearest Neighbors, and Neural Nets.

6
00:00:20.220 --> 00:00:21.675
For each of this family,

7
00:00:21.675 --> 00:00:25.295
I will give a short intuitive explanation with examples.

8
00:00:25.295 --> 00:00:27.660
If you don't remember any of these topics,

9
00:00:27.660 --> 00:00:32.630
I strongly encourage you to learn it using links from additional materials.

10
00:00:32.630 --> 00:00:34.910
Let's start with Linear Models.

11
00:00:34.910 --> 00:00:37.648
Imagine that we have two sets of points,

12
00:00:37.648 --> 00:00:41.640
gray points belong to one class and green ones to another.

13
00:00:41.640 --> 00:00:45.405
It is very intuitive to separate them with a line.

14
00:00:45.405 --> 00:00:49.810
In this case, it's quite simple to do since we have only two dimensional points.

15
00:00:49.810 --> 00:00:53.620
But this approach can be generalized for a high dimensional space.

16
00:00:53.620 --> 00:00:56.410
This is the main idea behind Linear Models.

17
00:00:56.410 --> 00:01:01.340
They try to separate objects with a plane which divides space into two parts.

18
00:01:01.340 --> 00:01:07.700
You can remember several examples from this model class like logistic regression or SVM.

19
00:01:07.700 --> 00:01:11.535
They all are Linear Models with different loss functions.

20
00:01:11.535 --> 00:01:13.700
I want to emphasize that Linear Models are

21
00:01:13.700 --> 00:01:17.005
especially good for sparse high dimensional data.

22
00:01:17.005 --> 00:01:20.295
But you should keep in mind the limitations of Linear Models.

23
00:01:20.295 --> 00:01:24.175
Often, point cannot be separated by such a simple approach.

24
00:01:24.175 --> 00:01:28.315
As an example, you can imagine two sets of points that form rings,

25
00:01:28.315 --> 00:01:30.085
one inside the other.

26
00:01:30.085 --> 00:01:33.005
Although it's pretty obvious how to separate them,

27
00:01:33.005 --> 00:01:37.490
Linear Models are not an appropriate choice either and will fail in this case.

28
00:01:37.490 --> 00:01:42.545
You can find implementations of Linear Models in almost every machine learning library.

29
00:01:42.545 --> 00:01:45.570
Most known implementation in Scikit-Learn library.

30
00:01:45.570 --> 00:01:49.120
Another implementation which deserves our attention is Vowpal Wabbit,

31
00:01:49.120 --> 00:01:52.585
because it is designed to handle really large data sets.

32
00:01:52.585 --> 00:01:57.485
We're finished with Linear Model here and move on to the next family, Tree-Based Methods.

33
00:01:57.485 --> 00:02:00.210
Tree-Based Methods use decision tree as

34
00:02:00.210 --> 00:02:03.175
a basic block for building more complicated models.

35
00:02:03.175 --> 00:02:06.185
Let's consider an example of how decision tree works.

36
00:02:06.185 --> 00:02:10.305
Imagine that we have two sets of points similar to a linear case.

37
00:02:10.305 --> 00:02:15.995
Let's separate one class from the other by a line parallel to the one of the axes.

38
00:02:15.995 --> 00:02:19.290
We use such restrictions as it significantly reduces the number of

39
00:02:19.290 --> 00:02:23.630
possible lines and allows us to describe the line in a simple way.

40
00:02:23.630 --> 00:02:26.253
After setting the split as shown at that picture,

41
00:02:26.253 --> 00:02:27.985
we will get two sub spaces,

42
00:02:27.985 --> 00:02:30.665
upper will have probability of gray=1,

43
00:02:30.665 --> 00:02:34.005
and lower will have probability of gray=0.2.

44
00:02:34.005 --> 00:02:37.330
Upper sub-space doesn't require any further splitting.

45
00:02:37.330 --> 00:02:40.300
Let's continue splitting for the lower sub-space.

46
00:02:40.300 --> 00:02:45.750
Now, we have zero probability on gray for the left sub-space and one for the right.

47
00:02:45.750 --> 00:02:49.220
This was a brief overview of how decision tree works.

48
00:02:49.220 --> 00:02:53.950
It uses divide-and-conquer approach to recur sub-split spaces into sub-spaces.

49
00:02:53.950 --> 00:02:57.770
Intuitively, single decision tree can be imagined as dividing space into

50
00:02:57.770 --> 00:03:02.500
boxes and approximating data with a constant inside of these boxes.

51
00:03:02.500 --> 00:03:05.410
The way of true axis splits and corresponding constants

52
00:03:05.410 --> 00:03:08.525
produces several approaches for building decision trees.

53
00:03:08.525 --> 00:03:12.695
Moreover, such trees can be combined together in a lot of ways.

54
00:03:12.695 --> 00:03:16.338
All this leads to a wide variety of tree-based algorithms,

55
00:03:16.338 --> 00:03:21.005
most famous of them being random forest and Gradient Boosted Decision Trees.

56
00:03:21.005 --> 00:03:23.075
In case if you don't know what are that,

57
00:03:23.075 --> 00:03:27.675
I strongly encourage you to remember these topics using links from additional materials.

58
00:03:27.675 --> 00:03:30.515
In general, tree-based models are very powerful

59
00:03:30.515 --> 00:03:33.320
and can be a good default method for tabular data.

60
00:03:33.320 --> 00:03:35.055
In almost every competitions,

61
00:03:35.055 --> 00:03:37.315
winners use this approach.

62
00:03:37.315 --> 00:03:39.290
But keep in mind that for Tree-Based Methods,

63
00:03:39.290 --> 00:03:44.870
it's hard to capture linear dependencies since it requires a lot of splits.

64
00:03:44.870 --> 00:03:49.535
We can imagine two sets of points which can be separated with a line.

65
00:03:49.535 --> 00:03:54.765
In this case, we need to grow a tree with a lot of splits in order to separate points.

66
00:03:54.765 --> 00:03:58.965
Even in such case, our tree could be inaccurate near decision border,

67
00:03:58.965 --> 00:04:00.640
as shown on the picture.

68
00:04:00.640 --> 00:04:02.731
Similar to Linear Models,

69
00:04:02.731 --> 00:04:04.030
you can find implementations of

70
00:04:04.030 --> 00:04:07.225
tree-based models in almost every machine learning library.

71
00:04:07.225 --> 00:04:10.010
Scikit-Learn contains quite good implementation of

72
00:04:10.010 --> 00:04:12.940
random forest which I personally prefer.

73
00:04:12.940 --> 00:04:17.385
All the Scikit-Learn contain implementation of gradient boost decision trees.

74
00:04:17.385 --> 00:04:23.260
I prefer to use libraries like XGBoost and LightGBM for their higher speed and accuracy.

75
00:04:23.260 --> 00:04:28.775
So, here we end the overview of Tree-Based Methods and move on to the k-NN.

76
00:04:28.775 --> 00:04:30.940
Before I start the explanation,

77
00:04:30.940 --> 00:04:35.040
I want to say that k-NN is abbreviation for k-Nearest Neighbors.

78
00:04:35.040 --> 00:04:37.815
One shouldn't mix it up with Neural Networks.

79
00:04:37.815 --> 00:04:41.790
So, let's take a look at the familiar binary classification problem.

80
00:04:41.790 --> 00:04:44.250
Imagine that we need to predict label for

81
00:04:44.250 --> 00:04:47.005
the points shown with question mark at this slide.

82
00:04:47.005 --> 00:04:52.765
We assume that points close to each other are likely to have similar labels.

83
00:04:52.765 --> 00:04:55.558
So, we need to find the closest point

84
00:04:55.558 --> 00:04:59.740
which displayed by arrow and pick its label as an answer.

85
00:04:59.740 --> 00:05:03.195
This is how nearest neighbor's method generally works.

86
00:05:03.195 --> 00:05:05.350
It can be easily generalized for k-NN,

87
00:05:05.350 --> 00:05:10.685
if we will find k-nearest objects and select plus labeled by majority vote.

88
00:05:10.685 --> 00:05:14.270
The intuition behind k-NN is very simple.

89
00:05:14.270 --> 00:05:17.250
Closer objects will likely to have same labels.

90
00:05:17.250 --> 00:05:19.165
In this particular example,

91
00:05:19.165 --> 00:05:22.350
we use square distance to find the closest object.

92
00:05:22.350 --> 00:05:26.175
In general case, it can be meaningless to use such a distance function.

93
00:05:26.175 --> 00:05:31.630
For example, square distance over images is unable to capture semantic meaning.

94
00:05:31.630 --> 00:05:34.070
Despite simplicity of the approach,

95
00:05:34.070 --> 00:05:37.520
features based on nearest neighbors are often very informative.

96
00:05:37.520 --> 00:05:41.065
We will discuss them in more details later in our course.

97
00:05:41.065 --> 00:05:45.830
Implementations of k-NN can be found in a lot of machine learning libraries.

98
00:05:45.830 --> 00:05:49.607
I suggest you to use implementation from Scikit-Learn since it use

99
00:05:49.607 --> 00:05:52.240
algorithm matrix to speedup recollections and

100
00:05:52.240 --> 00:05:55.740
allows you to use several predefined distance functions.

101
00:05:55.740 --> 00:05:59.195
Also, it allows you to implement your own distance function.

102
00:05:59.195 --> 00:06:03.520
The next big class of model I want to overview is Neural Networks.

103
00:06:03.520 --> 00:06:07.115
Neural Nets is a special class of machine learning models,

104
00:06:07.115 --> 00:06:09.310
which deserve a separate topic.

105
00:06:09.310 --> 00:06:12.425
In general, such methods can be seen in this Black-Box which produce

106
00:06:12.425 --> 00:06:16.570
a smooth separating curve in contrast to decision trees.

107
00:06:16.570 --> 00:06:20.390
I encourage you to visit TensorFlow playground which is shown on the slide,

108
00:06:20.390 --> 00:06:23.810
and play with different parameters of the simple feed-forward network in

109
00:06:23.810 --> 00:06:28.085
order to get some intuition about how feed-forward Neural Nets works.

110
00:06:28.085 --> 00:06:32.242
Some types of Neural Nets are especially good for images,

111
00:06:32.242 --> 00:06:35.085
sounds, text, and sequences.

112
00:06:35.085 --> 00:06:38.385
We won't cover details of Neural Nets in this course.

113
00:06:38.385 --> 00:06:42.400
Since Neural Nets attracted a lot of attention over the last few years,

114
00:06:42.400 --> 00:06:44.935
there are a lot of frameworks to work with them.

115
00:06:44.935 --> 00:06:47.360
Packages like TensorFlow, Keras,

116
00:06:47.360 --> 00:06:52.070
MXNet, PyTorch, and Lasagne can be used to feed Neural Nets.

117
00:06:52.070 --> 00:06:54.817
I personally prefer PyTorch since it's provides

118
00:06:54.817 --> 00:06:58.190
flexible and user-friendly way to define complex networks.

119
00:06:58.190 --> 00:06:59.865
After this brief recap,

120
00:06:59.865 --> 00:07:03.520
I want to say a few words about No Free Lunch Theorem.

121
00:07:03.520 --> 00:07:07.290
Basically, No Free Lunch Theorem states that there

122
00:07:07.290 --> 00:07:11.235
is no methods which outperform all others on all tasks,

123
00:07:11.235 --> 00:07:14.040
or in other words, for every method,

124
00:07:14.040 --> 00:07:19.395
we can construct a task for which this particular method will not be the best.

125
00:07:19.395 --> 00:07:24.505
The reason for that is that every method relies on some assumptions about data or task.

126
00:07:24.505 --> 00:07:26.640
If these assumptions fail,

127
00:07:26.640 --> 00:07:28.470
Limited will perform poorly.

128
00:07:28.470 --> 00:07:33.570
For us, this means that we cannot every competition with just a single algorithm.

129
00:07:33.570 --> 00:07:37.705
So we need to have a variety of tools based off different assumptions.

130
00:07:37.705 --> 00:07:39.510
Before the end of this video,

131
00:07:39.510 --> 00:07:42.477
I want to show you an example from Scikit-Learn library,

132
00:07:42.477 --> 00:07:46.115
which plots decision surfaces for different classifiers.

133
00:07:46.115 --> 00:07:48.706
We can see the type of algorithm have

134
00:07:48.706 --> 00:07:52.940
a significant influence of decision boundaries and consequently on [inaudible].

135
00:07:52.940 --> 00:07:57.170
I strongly suggest you to dive deeper into this example and make sure that you

136
00:07:57.170 --> 00:08:01.635
have intuition why these classifiers produce such surfaces.

137
00:08:01.635 --> 00:08:05.700
In the end, I want to remind you the main points of this video.

138
00:08:05.700 --> 00:08:08.590
First of all, there is no silver bullet algorithm which

139
00:08:08.590 --> 00:08:12.840
outperforms all the other in all and every task.

140
00:08:12.840 --> 00:08:15.714
Next, is that Linear Model can be imagined as splitting

141
00:08:15.714 --> 00:08:19.485
space into two sub-spaces separated by a hyper plane.

142
00:08:19.485 --> 00:08:26.145
Tree-Based Methods split space into boxes and use constant the predictions in every box.

143
00:08:26.145 --> 00:08:29.250
k-NN methods are based on the assumptions that

144
00:08:29.250 --> 00:08:32.220
close objects are likely to have same labels.

145
00:08:32.220 --> 00:08:35.685
So we need to find closest objects and pick their labels.

146
00:08:35.685 --> 00:08:40.075
Also, k-NN approach heavily relies on how to measure point closeness.

147
00:08:40.075 --> 00:08:43.135
Feed-forward Neural Nets are harder to interpret

148
00:08:43.135 --> 00:08:46.580
but they produce smooth non-linear decision boundary.

149
00:08:46.580 --> 00:08:51.630
The most powerful methods are Gradient Boosted Decision Trees and Neural Networks.

150
00:08:51.630 --> 00:08:53.595
But we shouldn't underestimate Linear Models

151
00:08:53.595 --> 00:08:56.795
and k-NN because sometimes, they may be better.

152
00:08:56.795 --> 00:09:02.350
We will show you relevant examples later in our course. Thank you for your attention.