WEBVTT

1
00:00:00.000 --> 00:00:03.260
[MUSIC]

2
00:00:03.260 --> 00:00:07.649
So in this video,
I will go through Springleaf data,

3
00:00:07.649 --> 00:00:11.260
it was a competition on Kaggle.

4
00:00:11.260 --> 00:00:15.430
In that competition,
the competitors were to

5
00:00:15.430 --> 00:00:20.070
predict whether a client will respond to
direct mail offer provided by Springleaf.

6
00:00:20.070 --> 00:00:22.940
So presumably,
we'll have some features about client,

7
00:00:22.940 --> 00:00:28.115
some features about offer, and we'll
need to predict 1 if he will respond and

8
00:00:28.115 --> 00:00:30.980
0 if he will not, so let's start.

9
00:00:30.980 --> 00:00:34.440
We'll first import some libraries in here,
define some functions,

10
00:00:34.440 --> 00:00:35.870
it's not very interesting.

11
00:00:35.870 --> 00:00:41.290
And finally, let's load the data and

12
00:00:41.290 --> 00:00:45.230
train our test one, and
do a little bit of data overview.

13
00:00:45.230 --> 00:00:50.679
So the first thing we want to know about
our data is the shapes of data tables,

14
00:00:50.679 --> 00:00:54.953
so let's bring the train shape,
and test that test shape.

15
00:00:54.953 --> 00:01:00.519
What we see here, we have one
150,000 objects, both in train and

16
00:01:00.519 --> 00:01:05.790
test sets, and about 2000
features in both train and test.

17
00:01:05.790 --> 00:01:10.672
And what we see more than,
we have one more feature in train, and

18
00:01:10.672 --> 00:01:15.120
as humans, just target can
continue to move the train.

19
00:01:15.120 --> 00:01:19.160
So we should just keep it in mind and
be careful, and

20
00:01:19.160 --> 00:01:21.820
drop this column when we feed our models.

21
00:01:23.420 --> 00:01:25.490
So let's examine training and test,

22
00:01:25.490 --> 00:01:30.770
so let's use this function had
to print several rows of both.

23
00:01:30.770 --> 00:01:35.884
We see here we have ID column, and
what's interesting here is that I see in

24
00:01:35.884 --> 00:01:41.422
training we have values 2, 4, 5,
7, and in test we have 1, 3, 6, 9.

25
00:01:41.422 --> 00:01:45.102
And it seems like they
are not overlapping, and

26
00:01:45.102 --> 00:01:49.153
I suppose the generation
process was as following.

27
00:01:49.153 --> 00:01:55.438
So the organizers created a huge
data set with 300,000 rules,

28
00:01:55.438 --> 00:02:01.630
and then they sampled at random,
rows for the train and for the test.

29
00:02:01.630 --> 00:02:05.747
And that is basically how we
get this train and test, and

30
00:02:05.747 --> 00:02:10.660
we have this column IG, it is row
index in this original huge file.

31
00:02:10.660 --> 00:02:16.197
Then we have something categorical,
then something numeric,

32
00:02:16.197 --> 00:02:22.970
numeric again, categorical, then
something that can be numeric or binary.

33
00:02:22.970 --> 00:02:27.791
But you see has decimal part,
so I don't know why,

34
00:02:27.791 --> 00:02:35.100
then some very strange values in here,
and again, something categorical.

35
00:02:35.100 --> 00:02:38.850
And actually,
we have a lot of in between, and yeah,

36
00:02:38.850 --> 00:02:44.740
we have target as the last column
of the train set, so let's move on.

37
00:02:44.740 --> 00:02:49.766
Probably another thing we want to
check is whether we have not a numbers

38
00:02:49.766 --> 00:02:55.180
in our data set, like nonce values,
and we can do it in several ways.

39
00:02:55.180 --> 00:02:58.250
And one way we,

40
00:02:58.250 --> 00:03:02.870
let's compute how many NaNs are there for
each object, for each row.

41
00:03:02.870 --> 00:03:06.870
So this is actually what we do here, and

42
00:03:06.870 --> 00:03:11.856
we print only the values for
the first 15 rows.

43
00:03:11.856 --> 00:03:15.502
And so the row 0 has 25 NaNs,

44
00:03:15.502 --> 00:03:19.848
row 1 has 19 NaN,, and so on, but

45
00:03:19.848 --> 00:03:26.180
what's interesting here,
six rows have 24 NaNs.

46
00:03:26.180 --> 00:03:29.245
It doesn't look like we got it in random,

47
00:03:29.245 --> 00:03:32.785
it's really unlikely to
have these at random.

48
00:03:32.785 --> 00:03:39.900
So my hypothesis could be that
the row order has some structure, so

49
00:03:39.900 --> 00:03:45.640
the rows are not shuffled, and
that is why we have this kind of pattern.

50
00:03:45.640 --> 00:03:50.813
And that means that we
probably could use row index

51
00:03:50.813 --> 00:03:56.592
as another feature for
our classifier, so that is it.

52
00:03:56.592 --> 00:04:01.430
And the same, we can do with columns, so

53
00:04:01.430 --> 00:04:05.280
for each column, let's compute how
many NaNs are there in each column.

54
00:04:05.280 --> 00:04:10.480
And we see that ID has 0 NaNs,
then some 0s, and

55
00:04:10.480 --> 00:04:16.330
then we see that a lot of
columns have the same 56 NaNs.

56
00:04:16.330 --> 00:04:21.350
And that is again something really
strange, so either every column will

57
00:04:21.350 --> 00:04:27.040
have 56 NaNs, and so it's not magic,
it's probably just how the things go.

58
00:04:27.040 --> 00:04:32.660
But if we know that there are a lot
of columns, and every column have

59
00:04:33.790 --> 00:04:38.119
more different number of NaNs, then it's
really unlikely to have a lot of columns

60
00:04:40.330 --> 00:04:44.380
nearer to each other in the data
set with the same number of NaNs.

61
00:04:44.380 --> 00:04:49.250
So probably, our hypothesis could
be here that the column order

62
00:04:49.250 --> 00:04:53.330
is not random, so
we could probably investigate this.

63
00:04:55.930 --> 00:04:59.600
So we have about 2,000
columns in this data, and

64
00:04:59.600 --> 00:05:02.312
it's a really huge number of columns.

65
00:05:02.312 --> 00:05:05.770
And it's really hard to work
with this data set, and

66
00:05:05.770 --> 00:05:08.340
basically we don't have any names,
so the data is only mice.

67
00:05:09.940 --> 00:05:13.314
As I told you,
the first thing we can do is

68
00:05:13.314 --> 00:05:18.098
to determine the types of the data,
so we will do it here.

69
00:05:18.098 --> 00:05:23.053
So we're first going to continue train and
test on a huge data frame

70
00:05:23.053 --> 00:05:27.845
like the organizers had,
it will have 300,000 rows.

71
00:05:27.845 --> 00:05:31.800
And then we'll first use
a unique function to

72
00:05:31.800 --> 00:05:36.284
determine how many unique
values each column has.

73
00:05:36.284 --> 00:05:41.742
And basically here we bring
several values of what we found,

74
00:05:41.742 --> 00:05:48.390
and it seems like there are five columns
that have only one unique number.

75
00:05:48.390 --> 00:05:52.021
So we can drop the,
basically what we have here,

76
00:05:52.021 --> 00:05:56.020
we just find them in this line,
and then we drop them.

77
00:05:56.020 --> 00:06:01.025
So next we want to remove
duplicated features, but first,

78
00:06:01.025 --> 00:06:05.829
for convenience,
fill not a numbers with something that

79
00:06:05.829 --> 00:06:10.550
we can find easily later, and
then we do the following.

80
00:06:10.550 --> 00:06:14.494
So we create another data frame of size,

81
00:06:14.494 --> 00:06:18.327
of a similar shape as the training set.

82
00:06:18.327 --> 00:06:22.089
What we do we take
a column from train set,

83
00:06:22.089 --> 00:06:27.788
we apply a label encoder,
as we discussed in a previous video,

84
00:06:27.788 --> 00:06:32.114
and we basically store it
in this new train set.

85
00:06:32.114 --> 00:06:34.940
So basically we get another
data frame which is train,

86
00:06:34.940 --> 00:06:38.440
but label encoded train set.

87
00:06:38.440 --> 00:06:44.340
And having this data frame,
we can easily find duplicated features,

88
00:06:44.340 --> 00:06:49.250
we just start iterating
the features with two iterators.

89
00:06:49.250 --> 00:06:54.580
Basically, one is fixed and the second one
goes from the next feature to the end.

90
00:06:56.470 --> 00:07:02.340
Then we try to compare the columns, the
two columns that we're standing at, right.

91
00:07:02.340 --> 00:07:07.640
And if they are element wise the same,
then we have duplicated columns,

92
00:07:07.640 --> 00:07:13.030
and basically that is how we fill up
this dictionary of duplicated columns.

93
00:07:13.030 --> 00:07:18.300
We see it here, so
we found that variable 9 is duplicated for

94
00:07:18.300 --> 00:07:22.310
input 8, and
variable 18 again is duplicated for

95
00:07:22.310 --> 00:07:25.850
variable 8, and so on, and so we have
really a lot of duplicates in here.

96
00:07:27.900 --> 00:07:33.540
So this loop, it took some time,
so I prefer to dump

97
00:07:33.540 --> 00:07:38.550
the results to disk, so
we can easily restore them.

98
00:07:38.550 --> 00:07:43.265
So I do it here, and
then I basically drop those columns

99
00:07:43.265 --> 00:07:47.170
that we found from
the train test data frame.

100
00:07:47.170 --> 00:07:51.330
So yeah, in the second video,

101
00:07:51.330 --> 00:07:56.131
we will go through some features and

102
00:07:56.131 --> 00:07:59.500
do some work to data set.

103
00:07:59.500 --> 00:08:09.500
[MUSIC]