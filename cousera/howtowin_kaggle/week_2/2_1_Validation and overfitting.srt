1
00:00:02.430 --> 00:00:06.910
これは、大会でまれな場合は、参照してくだ
さいされていません

2
00:00:06.910 --> 00:00:10.800
個人の結果を明らかにした後、人々はリーダ
ーボードにジャンプします。

3
00:00:10.800 --> 00:00:12.682
だから、我々は自問し、

4
00:00:12.682 --> 00:00:14.125
何が起きてるんだ?

5
00:00:14.125 --> 00:00:16.695
これらのジャンプの2つの主な理由がありま
す。

6
00:00:16.695 --> 00:00:19.960
まず、競合他社の検証を無視し、

7
00:00:19.960 --> 00:00:23.268
公開スコアボードに対してベストを挙げた提
出書類を選択します。

8
00:00:23.268 --> 00:00:26.710
第二に、時には大会がある

9
00:00:26.710 --> 00:00:30.250
一貫性のないパブリック/プライベートデー
タの分割または

10
00:00:30.250 --> 00:00:34.725
パブリックまたはプライベートのスコアボー
ドのデータが少なすぎます。

11
00:00:34.725 --> 00:00:36.730
さて、我々は参加者として、

12
00:00:36.730 --> 00:00:39.125
コンペティション組織に影響を与えることは
できません。

13
00:00:39.125 --> 00:00:41.800
我々は確かに我々が選択することを確認する
ことができます

14
00:00:41.800 --> 00:00:45.929
当社の最も適切な提出は、プライベートリー
ダーボードによって評価される。

15
00:00:45.929 --> 00:00:49.780
したがって、次のビデオの広い目標は

16
00:00:49.780 --> 00:00:53.980
競争で検証を設定する体系的な方法を提供す
るには、

17
00:00:53.980 --> 00:00:56.840
、最も一般的な検証の問題に対処します。

18
00:00:56.840 --> 00:01:01.220
次の動画の内容の概要を簡単に説明しましょ
う。

19
00:01:01.220 --> 00:01:03.140
まず、このビデオでは、

20
00:01:03.140 --> 00:01:06.895
我々は、検証とオーバーフィット回避の概念
を理解します。

21
00:01:06.895 --> 00:01:08.515
2番目のビデオでは、

22
00:01:08.515 --> 00:01:13.815
安定した検証を確立するために実行する必要
がある分割の数を特定します。

23
00:01:13.815 --> 00:01:16.430
3番目のビデオでは、我々が通過する

24
00:01:16.430 --> 00:01:21.290
競技での列車/テストの分割を行うために使
用される最も頻繁な方法.

25
00:01:21.290 --> 00:01:22.549
最後のビデオでは、

26
00:01:22.549 --> 00:01:26.060
ほとんどの場合、検証の問題について説明し
ます。

27
00:01:26.060 --> 00:01:27.875
さあ、説明を始めよう

28
00:01:27.875 --> 00:01:31.325
それを聞いたことがない人のための検証のた
めの概念。

29
00:01:31.325 --> 00:01:36.754
一言で言えば、我々はモデルが目に見えない
データに期待される結果を与えるかどうかを
確認したい。

30
00:01:36.754 --> 00:01:39.410
たとえば、で働いている場合

31
00:01:39.410 --> 00:01:42.880
患者の生活を改善することを目標とするヘル
スケア企業

32
00:01:42.880 --> 00:01:46.490
我々は、患者が予測するタスクを与えること
ができる

33
00:01:46.490 --> 00:01:50.525
近い将来、特定の病気と診断される。

34
00:01:50.525 --> 00:01:55.505
ここでは、我々は、我々が将来的に適用され
る列車のモデルであることを確認する必要が
あります。

35
00:01:55.505 --> 00:01:56.915
適当なだけでなく、

36
00:01:56.915 --> 00:01:59.900
我々は、このモデルがどのような品質につい
て確認する必要があります

37
00:01:59.900 --> 00:02:03.950
モデルが作る間違いの数に応じて持っている
。

38
00:02:03.950 --> 00:02:08.495
この特定の疾患を有する患者の予測確率につ
いて、

39
00:02:08.495 --> 00:02:10.340
我々は実行することを決定する必要がありま
す

40
00:02:10.340 --> 00:02:14.910
診断を明確にするために患者のための特別な
医学のテスト。

41
00:02:14.910 --> 00:02:19.570
したがって、我々は正しく我々のモデルの品
質を理解する必要があります。

42
00:02:19.570 --> 00:02:23.000
しかし、この品質は、から列車のデータに異
なることができます

43
00:02:23.000 --> 00:02:27.748
過去と未来から目に見えないテストデータに
。

44
00:02:27.748 --> 00:02:31.460
モデルは、単に列車のデータからすべての患
者を暗記することができる

45
00:02:31.460 --> 00:02:35.950
テストデータでは完全に役に立たないので、
これは発生したくない。

46
00:02:35.950 --> 00:02:38.870
我々は、データを使用してモデルの品質を確
認する必要があります

47
00:02:38.870 --> 00:02:42.500
があり、これらのチェックは検証です。

48
00:02:42.500 --> 00:02:47.670
だから、通常、我々は2つの部分に持ってい
るデータを分割し、

49
00:02:47.670 --> 00:02:50.280
パーツと検証パーツをトレーニングします。

50
00:02:50.280 --> 00:02:56.135
我々は、列車の部分に私たちのモデルに適合
し、検証の部分でその品質を確認してくださ
い。

51
00:02:56.135 --> 00:02:58.380
その横で、最後の例では、

52
00:02:58.380 --> 00:03:01.890
私たちのモデルは、目に見えないデータに対
してチェックされます

53
00:03:01.890 --> 00:03:06.140
未来と実際にこれらのデータは、我々が持っ
ているデータと異なることができます。

54
00:03:06.140 --> 00:03:08.160
だから準備しておくべきだ

55
00:03:08.160 --> 00:03:12.165
コンクールでは、普通に似たような状況があ
ります。

56
00:03:12.165 --> 00:03:16.735
競争の主催者は私たちに2つのチャンクのデ
ータを提供します。

57
00:03:16.735 --> 00:03:20.070
まず、すべてのターゲット値を使用してデー
タをトレーニングします。

58
00:03:20.070 --> 00:03:22.950
次に、ターゲット値のないデータをテストし
ます。

59
00:03:22.950 --> 00:03:24.585
前の例のように、

60
00:03:24.585 --> 00:03:27.818
我々は、列車と検証の部分にラベルを使用し
てデータを分割する必要があります。

61
00:03:27.818 --> 00:03:33.030
さらに、競争の精神を確保するために、

62
00:03:33.030 --> 00:03:38.346
主催者は、テストデータを公開テストセット
とプライベートテストセットに分割します。

63
00:03:38.346 --> 00:03:41.143
私達がプラットホームに私達の服従を送った
ときに、

64
00:03:41.143 --> 00:03:45.150
我々は、パブリックテストのスコアを設定し
ながら、スコアを参照してください

65
00:03:45.150 --> 00:03:50.120
プライベートテストセットは、コンペティシ
ョン終了後にのみリリースされます。

66
00:03:50.120 --> 00:03:56.489
これはまた、我々はテストセットを必要とし
ないか、またはモデルの面で
overfit しないことを保証します。

67
00:03:56.489 --> 00:03:59.995
私はあなたの病気の投影とのアナロジーを描
画させて、

68
00:03:59.995 --> 00:04:04.545
我々はすでに列車と検証部品に我々のデータ
を分割した場合。

69
00:04:04.545 --> 00:04:08.925
そして今、我々は繰り返し検証セットに対し
て我々のモデルをチェックしている、

70
00:04:08.925 --> 00:04:11.010
いくつかのモデルは、偶然だけで、

71
00:04:11.010 --> 00:04:13.475
他のものより良いスコアを持つことになりま
す。

72
00:04:13.475 --> 00:04:16.808
我々は最高のモデルを選択し続ける場合,
それらを変更する,

73
00:04:16.808 --> 00:04:19.007
そして再び彼らからベストを選択し、

74
00:04:19.007 --> 00:04:22.170
我々は、スコアの一定の改善が表示されます
。

75
00:04:22.170 --> 00:04:26.295
しかし、それは我々が将来のテストデータに
これらの改善が表示されますことを意味しま
せん。

76
00:04:26.295 --> 00:04:29.105
これを何度も繰り返して、

77
00:04:29.105 --> 00:04:33.555
我々は、検証セットを達成したり、競争の面
で、

78
00:04:33.555 --> 00:04:36.095
我々は、公開ランキングをごまかすことがで
きます。

79
00:04:36.095 --> 00:04:37.800
でもやはり、overfit ば、

80
00:04:37.800 --> 00:04:40.930
プライベートリーダーボードは、私たちがダ
ウンできるようになります。

81
00:04:40.930 --> 00:04:44.185
これは我々が競争の中でオーバーフィット回
避と呼ぶものです。

82
00:04:44.185 --> 00:04:46.890
非現実的良いスコアを取得する

83
00:04:46.890 --> 00:04:52.390
公開スコアボードは、後でプライベートスコ
アボードをジャンプすることになります。

84
00:04:52.390 --> 00:04:56.850
したがって、我々のモデルは、パターンをキ
ャプチャすることができるようにしたい

85
00:04:56.850 --> 00:05:02.670
データだけでなく、両方の列車とテストデー
タの間に一般化するこれらのパターン。

86
00:05:02.670 --> 00:05:06.834
私は underfitting とオーバ
ーフィット回避の観点から、このプロセスを
お見せしましょう。

87
00:05:06.834 --> 00:05:09.180
だから、最高のモデルを選択するには、

88
00:05:09.180 --> 00:05:14.130
我々は基本的に片側に
underfitting を避けるために
、他の上にオーバーフィット回避したい。

89
00:05:14.130 --> 00:05:19.430
この概念を、バイナリ分類テストの非常に単
純な例で理解してみましょう。

90
00:05:19.430 --> 00:05:23.070
我々は、下の数式で定義された単純なモデル
を使用する

91
00:05:23.070 --> 00:05:27.730
画像とモデルの予測の結果を視覚化します。

92
00:05:27.730 --> 00:05:29.310
ここで左の画像で、

93
00:05:29.310 --> 00:05:32.555
我々は、モデルがあまりにも単純な場合は、
見ることができます

94
00:05:32.555 --> 00:05:37.500
それは下線付きの関係を捕獲できないし、私
達は悪い結果を得る。

95
00:05:37.500 --> 00:05:38.520
これを underfitting
といいます。

96
00:05:38.520 --> 00:05:42.345
結果を改善したいなら

97
00:05:42.345 --> 00:05:45.660
我々は、モデルの複雑さを高めることができ
ます

98
00:05:45.660 --> 00:05:50.750
それはおそらく、トレーニングデータの品質
が下がっていることがわかります。

99
00:05:50.750 --> 00:05:55.045
しかし、一方で、我々は右の画像のようにあ
まりにも複雑なモデルを作る場合、

100
00:05:55.045 --> 00:06:00.960
それはテストデータを一般化しない列車デー
タの騒音を記述し始める。

101
00:06:00.960 --> 00:06:04.925
そして、これはモデルの品質の低下につなが
る。

102
00:06:04.925 --> 00:06:06.790
これをオーバーフィット回避といいます。

103
00:06:06.790 --> 00:06:12.275
underfitting とオーバーフィ
ット回避の間に何かが欲しいんだ

104
00:06:12.275 --> 00:06:15.270
そして、最も適したモデルを選ぶことを目的
として、

105
00:06:15.270 --> 00:06:19.080
結果を評価できるようにしたい。

106
00:06:19.080 --> 00:06:21.030
ここでは、我々は、発言をする必要がありま
す

107
00:06:21.030 --> 00:06:25.020
機械学習におけるオーバーフィット回避の意
味

108
00:06:25.020 --> 00:06:29.970
特にオーバーフィット回避競技の意味は若干
異なる。

109
00:06:29.970 --> 00:06:33.180
一般的に、我々は、モデルが
overfitted であると言う

110
00:06:33.180 --> 00:06:37.265
列車セットの品質は、テストセットよりも優
れています。

111
00:06:37.265 --> 00:06:39.735
でもコンクールでは、よく言いますが、

112
00:06:39.735 --> 00:06:42.540
モデルが overfitted
場合にのみ

113
00:06:42.540 --> 00:06:46.970
テストセットの品質は、予想以上に悪化しま
す。

114
00:06:46.970 --> 00:06:49.040
たとえば、勾配ブーストデシジョンツリーを
トレーニングする場合は、

115
00:06:49.040 --> 00:06:53.280
競争はカーブの下で私達の区域であるメート
ル.

116
00:06:53.280 --> 00:06:56.040
私たちは時々観察することができます上の品
質

117
00:06:56.040 --> 00:06:59.250
トレーニングデータは、テストデータ上で1
つに近いですが、

118
00:06:59.250 --> 00:07:02.345
それは0.9
の近く、例えば、以下の可能性があります。

119
00:07:02.345 --> 00:07:07.020
一般的な意味では、モデルはここ
overfitted
が、我々はエリアを取得しながら

120
00:07:07.020 --> 00:07:12.810
曲線の下で検証とパブリック/プライベート
テストセットの両方で0.9 だった

121
00:07:12.810 --> 00:07:17.800
我々は、それが競争の文脈で
overfitted
であると言うことはありません。

122
00:07:17.800 --> 00:07:21.860
私はもう少し別の方法でこの概念を説明して
みましょう。

123
00:07:21.860 --> 00:07:25.200
だから、モデルの評価の目的のために言うこ
とができます

124
00:07:25.200 --> 00:07:28.330
我々は2つの部分にデータを分けた。

125
00:07:28.330 --> 00:07:30.510
列車と検証の部品。

126
00:07:30.510 --> 00:07:32.135
我々はすでにしたように、

127
00:07:32.135 --> 00:07:37.427
モデルの複雑さを低から高に導き、ここでモ
デルを見ていきます。

128
00:07:37.427 --> 00:07:41.130
注意してください, それは通常,
我々は理解する

129
00:07:41.130 --> 00:07:45.955
エラーまたは損失は、モデルの品質やスコア
に反対しているものです。

130
00:07:45.955 --> 00:07:49.530
図では、依存関係はかなり合理的に見えます
。

131
00:07:49.530 --> 00:07:50.955
2つの単純なモデルについては、

132
00:07:50.955 --> 00:07:55.590
我々は、列車と検証の両方で高いことを意味
underfitting
を持っています。

133
00:07:55.590 --> 00:07:57.070
2つの複雑なモデルでは、

134
00:07:57.070 --> 00:08:03.030
我々は、列車の低エラーを意味するオーバー
フィット回避を持っているが、検証上の再び
高いエラー。

135
00:08:03.030 --> 00:08:04.960
中には、その間に、

136
00:08:04.960 --> 00:08:06.960
完璧なモデルの複雑さなら、

137
00:08:06.960 --> 00:08:09.900
これは、検証データに最も低い電車を持って
おり、

138
00:08:09.900 --> 00:08:14.370
したがって、我々はそれが目に見えないテス
トデータの最も低いエラーを持っていること
を期待.

139
00:08:14.370 --> 00:08:18.000
注: ここでは、トレーニングエラーは常に

140
00:08:18.000 --> 00:08:22.800
一般的な意味でオーバーフィット回避を意味
するテストエラーよりも、

141
00:08:22.800 --> 00:08:26.340
しかし、大会の文脈では適用されません。

142
00:08:26.340 --> 00:08:27.615
よくやりましたね。

143
00:08:27.615 --> 00:08:30.015
このビデオでは、検証を定義し、

144
00:08:30.015 --> 00:08:32.857
その目的を示し、解釈された検証

145
00:08:32.857 --> 00:08:36.330
underfitting
とオーバーフィット回避の面で。

146
00:08:36.330 --> 00:08:39.045
だから、もう一度、一般的には、

147
00:08:39.045 --> 00:08:41.535
検証は、私たちが質問に答えるのに役立ちま
す、

148
00:08:41.535 --> 00:08:45.960
どのような unseeing のデータと
ヘルプの私達のモデルの品質になります

149
00:08:45.960 --> 00:08:52.130
我々は、テストデータの最高の品質を得るた
めに期待されるモデルを選択します。

150
00:08:52.130 --> 00:08:56.185
通常、我々はされている片側に
underfitting
を避けるためにしようとしている

151
00:08:56.185 --> 00:09:01.105
私たちのモデルは、データのパターンをキャ
プチャするのに十分な表現力を持つようにし
ます。

152
00:09:01.105 --> 00:09:04.260
そして、我々は反対側にオーバーフィット回
避を避けるために、しようとしている

153
00:09:04.260 --> 00:09:06.935
そして、あまりにも複雑なモデルを作成しな
いでください

154
00:09:06.935 --> 00:09:08.265
その場合、

155
00:09:08.265 --> 00:09:14.210
我々は、テストデータに一般化していないノ
イズやパターンをキャプチャするために開始
されます。

