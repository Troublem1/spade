1
00:00:00.000 --> 00:00:03.585
音楽

2
00:00:03.585 --> 00:00:06.102
これと次のビデオでは、我々は、議論する

3
00:00:06.102 --> 00:00:10.090
分類基準を最適化する方法は何ですか。

4
00:00:10.090 --> 00:00:13.280
このビデオでは、logloss
と精度について説明し、

5
00:00:13.280 --> 00:00:16.850
次のいずれかで, AUC
と二次加重カッパ.

6
00:00:16.850 --> 00:00:20.480
logloss
から始めましょう、logloss
のための

7
00:00:20.480 --> 00:00:26.040
分類は、攻撃のための MSE のようなも
のです、それはどこでも実装されます。

8
00:00:26.040 --> 00:00:30.070
必要なのは、ライブラリに渡す引数を調べる
ことだけです。

9
00:00:30.070 --> 00:00:31.740
それは訓練のために logloss
を使用するようにする。

10
00:00:33.380 --> 00:00:38.074
XGBoost、LightGBM のよう
に、試みるべき膨大な数の図書館があります
、

11
00:00:38.074 --> 00:00:43.489
ロジスティック回帰, sklearn
からの [聞こえない] 分類子,
Vowpal Wabbit.

12
00:00:43.489 --> 00:00:47.520
すべてのニューラルネットは、デフォルトで
は、分類のための logloss
を最適化します。

13
00:00:48.550 --> 00:00:54.280
ランダムフォレスト分類子の予測は、log
loss の面でかなり悪いことが判明。

14
00:00:54.280 --> 00:00:57.050
しかし、それらをより良くする方法がありま
す、

15
00:00:57.050 --> 00:01:02.530
我々はより良い logloss
に合わせて予測を調整することができます。

16
00:01:02.530 --> 00:01:06.010
我々は、logloss が出力するモデル
を必要とすることを数回言及しました

17
00:01:06.010 --> 00:01:08.610
外部確率が、それは何を意味するのですか?

18
00:01:09.850 --> 00:01:14.558
それは実際に我々は、例えば、のスコアを持
っているすべてのポイントを取る場合、意味

19
00:01:14.558 --> 00:01:22.171
0.8、その後、正確にネガよりも4倍以上
の肯定的なオブジェクトになります。

20
00:01:22.171 --> 00:01:28.910
つまり、ポイントの 80%
はクラス1から、クラス0から 20%
になります。

21
00:01:28.910 --> 00:01:32.945
分類子が logloss
を直接最適化しない場合は、

22
00:01:32.945 --> 00:01:35.420
その予測を校正する必要があります。

23
00:01:36.670 --> 00:01:42.080
このプロットを見てみると、青色の行は、値
の予測によって並べ替えを示しています

24
00:01:42.080 --> 00:01:43.800
検証セット。

25
00:01:43.800 --> 00:01:48.239
赤線は、対応するターゲット値をスムージン
グします。

26
00:01:48.239 --> 00:01:49.905
ローリングウィンドウを使用します。

27
00:01:49.905 --> 00:01:53.430
我々は明らかに我々の予測は、保守的なの一
種であることがわかります。

28
00:01:53.430 --> 00:01:57.400
theyツエre は、左側の2つのターゲ
ットの平均よりもはるかに大きい、と

29
00:01:57.400 --> 00:01:59.800
彼らは右側にする必要がありますよりもはる
かに低い。

30
00:02:01.000 --> 00:02:04.350
したがって、この分類子はキャリブレーショ
ンされず、

31
00:02:04.350 --> 00:02:07.350
緑色の曲線は、キャリブレーション後の予測
を示しています。

32
00:02:08.630 --> 00:02:11.340
しかし、もし我々のための並べ替えの予測を
プロット

33
00:02:11.340 --> 00:02:17.000
校正された分類器は、曲線は、ターゲットロ
ーリング平均に非常に似ています。

34
00:02:17.000 --> 00:02:20.840
そして実際には、キャリブレータの予測は、
ログの損失が低くなります。

35
00:02:21.950 --> 00:02:26.050
たとえば、予測を調整する方法はいくつかあ
ります。

36
00:02:26.050 --> 00:02:29.340
我々は、いわゆるプラットスケーリングを使
用することができます。

37
00:02:29.340 --> 00:02:33.039
基本的に、我々はちょうど我々の予測にロジ
スティック回帰に適合する必要があります。

38
00:02:34.280 --> 00:02:38.940
私は詳細にどのように行うには、行くことは
ありませんが、それは非常にどのように似て
いる

39
00:02:38.940 --> 00:02:44.967
スタックモデル, と我々は別のビデオで詳
細に積み重ね議論する.

40
00:02:44.967 --> 00:02:49.490
第二に、我々は、我々の予測に張回帰に適合
することができます

41
00:02:49.490 --> 00:02:53.715
繰り返しになりますが、それは非常にスタッ
キング、ちょうど別のモデルに似て行われま
す。

42
00:02:53.715 --> 00:02:58.490
最終的には、スタッキングを使用することが
できますが、

43
00:02:58.490 --> 00:03:01.861
だから、アイデアは、我々は任意の分類に適
合することができます。

44
00:03:01.861 --> 00:03:06.460
それは logloss を最適化する必要
はありません、それだけで良いことが必要で
す

45
00:03:06.460 --> 00:03:08.145
例では、AUC の面で。

46
00:03:09.550 --> 00:03:11.940
そして、我々は上に別のモデルを収めること
ができます

47
00:03:13.020 --> 00:03:17.620
それは私達のモデルの予測を取り、適切にそ
れらを調整します。

48
00:03:17.620 --> 00:03:21.990
そして、上のモデルは、その最適化の損失と
して logloss を使用します。

49
00:03:21.990 --> 00:03:27.320
だから、間接的に最適化され、その予測はキ
ャリブレーションされます。

50
00:03:28.575 --> 00:03:32.960
Logloss は、直接最適化するのが簡
単な唯一の指標でした。

51
00:03:32.960 --> 00:03:38.930
正確には、どのように直接それを最適化する
簡単なレシピはありません。

52
00:03:38.930 --> 00:03:43.930
一般的には、レシピは、実際には、バイナリ
の場合、次のとおりです

53
00:03:43.930 --> 00:03:48.666
分類タスク、任意のメトリックに適合し、二
値化のしきい値を調整します。

54
00:03:48.666 --> 00:03:53.820
マルチクラスのタスクの場合は、任意のメト
リックに適合し、

55
00:03:53.820 --> 00:03:58.430
モデルを精度スコアで比較するパラメータを
チューニングします。

56
00:03:58.430 --> 00:04:03.120
ではなく、メトリックは、モデルが本当に最
適化された。

57
00:04:04.230 --> 00:04:06.370
だから、これは早期停止のようなものであり
、

58
00:04:06.370 --> 00:04:09.840
精度スコアを調べるクロス検証。

59
00:04:11.020 --> 00:04:15.150
精度が最適化されにくい理由を直感で得るた
めに、このプロットを見てみましょう。

60
00:04:16.210 --> 00:04:19.900
だから、縦軸に我々は損失が表示されます,
と

61
00:04:19.900 --> 00:04:24.890
横軸は、例えば、決定境界への符号付き距離
を示します。

62
00:04:24.890 --> 00:04:27.830
ハイパープレーンまたは線形モデルの場合。

63
00:04:27.830 --> 00:04:32.920
クラスが正しく予測されている場合、距離は
正であると見なされます。

64
00:04:32.920 --> 00:04:37.370
オブジェクトが決定境界の間違った側にある
場合は、負の数になります。

65
00:04:38.530 --> 00:04:41.730
ここの青い線はゼロ1の損失を示しています

66
00:04:41.730 --> 00:04:44.970
これは、精度スコアに対応する損失です。

67
00:04:44.970 --> 00:04:49.130
我々は、オブジェクトがより、つまり、1を
支払う

68
00:04:49.130 --> 00:04:53.130
オブジェクトに負の距離があり、それ以外の
場合は何も払いません。

69
00:04:54.150 --> 00:04:58.610
問題は、この損失はほぼどこでもグラデーシ
ョンをゼロにしている、

70
00:04:58.610 --> 00:05:00.812
予測に関しては。

71
00:05:00.812 --> 00:05:06.537
そして、ほとんどの学習アルゴリズムは、そ
れ以外の場合に合わせて0以外のグラデーシ
ョンが必要

72
00:05:06.537 --> 00:05:12.719
それはどのように我々は損失が減少するよう
な予測を変更する必要が明確ではない。

73
00:05:14.050 --> 00:05:16.520
そして、人々はプロキシの損失を思い付いた

74
00:05:17.710 --> 00:05:22.120
これは、これらのゼロ1の損失の上限です。

75
00:05:22.120 --> 00:05:26.740
あなたが完全にプロキシの損失に適合するの
であれば、精度も完璧になります

76
00:05:28.120 --> 00:05:33.120
しかし、異なるゼロに1つの損失、彼らは微
分です。

77
00:05:33.120 --> 00:05:38.169
たとえば、ここでは、ロジスティック損失、
赤い曲線を使用して参照してください

78
00:05:38.169 --> 00:05:43.218
ロジスティック回帰、およびヒンジ損失では
、SVM で使用される損失。

79
00:05:45.300 --> 00:05:50.645
今、テストオブジェクトのハードラベルを取
得することを思い出して、我々は通常取る

80
00:05:50.645 --> 00:05:56.185
私たちのソフトの予測の argmax、最
大スコアを持つクラスを選ぶ。

81
00:05:56.185 --> 00:05:59.822
私たちのタスクは、バイナリとソフトの予測
は1まで合計されている場合、

82
00:05:59.822 --> 00:06:03.780
argmax はしきい値関数と等価です。

83
00:06:03.780 --> 00:06:08.880
出力1クラス1の予測が0.5
より高い場合、

84
00:06:08.880 --> 00:06:12.110
予測が低い場合は0を出力します。

85
00:06:13.490 --> 00:06:18.620
だから我々はすでにしきい値0.5
が最適ではないこの例を見てきました,

86
00:06:20.280 --> 00:06:22.550
それでどうする?

87
00:06:22.550 --> 00:06:24.290
我々は、適用されるしきい値を調整すること
ができます

88
00:06:24.290 --> 00:06:29.095
for ループで実装された単純なグリッド
検索で実行できます。

89
00:06:29.095 --> 00:06:35.650
まあ、それは我々が基本的に十分に強力なモ
デルに合うことができることを意味します。

90
00:06:35.650 --> 00:06:39.840
それはどのような損失を正確に、言う、ヒン
ジや問題ではない

91
00:06:39.840 --> 00:06:42.400
モデルが最適化するログの損失。

92
00:06:42.400 --> 00:06:44.910
私達が私達のモデルの予言からほしいすべて
は

93
00:06:44.910 --> 00:06:48.270
クラスを分離する良いしきい値の存在。

94
00:06:49.620 --> 00:06:53.670
また、我々の分類器が理想的に較正されてい
る場合、

95
00:06:53.670 --> 00:06:58.414
それは本当に後部確率を返しています。

96
00:06:58.414 --> 00:07:04.016
このような分類器については、閾値0.5
が最適であり、

97
00:07:04.016 --> 00:07:10.877
しかし、このような分類子はほとんどの場合
、しきい値のチューニングは頻繁に役立ちま
す。

98
00:07:10.877 --> 00:07:15.907
そこでこのビデオでは、我々は
logloss と議論

99
00:07:15.907 --> 00:07:21.916
精度は、次のビデオでは、AUC
と議論する

100
00:07:21.916 --> 00:07:25.285
二次加重カッパ.

101
00:07:25.285 --> 00:07:29.166
音楽

